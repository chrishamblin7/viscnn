{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/viscnn/lib/python3.7/site-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "Tesla K40c with CUDA capability sm_35 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the Tesla K40c GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "from viscnn.model_prep.utils import load_prepped_model_params, load_prepped_model\n",
    "\n",
    "model = load_prepped_model('alexnet_sparse')\n",
    "params = load_prepped_model_params('alexnet_sparse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA LOADER###\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "from viscnn.data_loading import rank_image_data\n",
    "from viscnn.dissected_Conv2d import *\n",
    "from copy import deepcopy\n",
    "\n",
    "kwargs = {'num_workers': params['num_workers'], 'pin_memory': True, 'sampler':None} if 'cuda' in params['device'] else {}\n",
    "\n",
    "\n",
    "image_loader = data.DataLoader(rank_image_data('/mnt/data/chris/dropbox/Research-Hamblin/Projects/cnn_subgraph_visualizer/image_data/alexnet10/ranking_images',\n",
    "                                                params['preprocess'],\n",
    "                                                label_file_path = '/mnt/data/chris/dropbox/Research-Hamblin/Projects/cnn_subgraph_visualizer/image_data/imagenet_50/labels.txt',class_folders=True),\n",
    "                                                batch_size=250,\n",
    "                                                shuffle=False,\n",
    "                                                **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add grasp modules\n",
    "net = dissect_model(deepcopy(model),device='cuda:0',dissect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import types\n",
    "\n",
    "\n",
    "from viscnn.utils import nodeid_2_perlayerid\n",
    "\n",
    "def count_total_parameters(net):\n",
    "    total = 0\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            total += m.weight.numel()\n",
    "    return total\n",
    "\n",
    "\n",
    "def count_fc_parameters(net):\n",
    "    total = 0\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "            total += m.weight.numel()\n",
    "    return total\n",
    "\n",
    "\n",
    "def feature_GraSP(net, target_node, ratio, dataloader, device, num_iters= None, eps = 1e-10, intv=20, params=None, rank_field = 'image'):\n",
    "    \n",
    "    \n",
    "    #prep model to stop at feature\n",
    "    net = dissect_model(net,device=device,dissect=False)\n",
    "    \n",
    "    if target_node != 'loss':\n",
    "        if not isinstance(target_node,list) or isinstance(target_node,tuple): \n",
    "            #convert nodeid to perlayer_id\n",
    "            target_node_layer,target_node_within_layer_id,target_node_layer_name = nodeid_2_perlayerid(target_node,params)\n",
    "            net=set_model_target_node(net,target_node_layer,target_node_within_layer_id)\n",
    "        else:\n",
    "            target_node_layer_name,target_node_within_layer_id = target_node[0], target_node[1] \n",
    "            net=set_model_target_node(net,target_node_layer_name,target_node_within_layer_id)\n",
    "        print('getting scores for target %s:%s'%(target_node_layer_name,str(target_node_within_layer_id)))\n",
    "\n",
    "    #set up loss\n",
    "    if params is not None:\n",
    "        criterion = params['criterion']\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "                \n",
    "    \n",
    "    keep_ratio = 1-ratio\n",
    "    old_net = net\n",
    "\n",
    "    net = copy.deepcopy(net)\n",
    "    net.zero_grad()\n",
    "    weights = []\n",
    "    total_parameters = count_total_parameters(net)\n",
    "    fc_parameters = count_fc_parameters(net)\n",
    "\n",
    "    '''\n",
    "    fc_layers = []\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                fc_layers.append(layer)\n",
    "            weights.append(layer.weight)\n",
    "    '''\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, hooked_Conv2d):\n",
    "            weights.append(layer.from_conv.weight)\n",
    "            if layer.name == target_node_layer_name:\n",
    "                break\n",
    "\n",
    "    inputs_one = []\n",
    "    targets_one = []\n",
    "\n",
    "    grad_w = None\n",
    "    grad_f = None\n",
    "    for w in weights:\n",
    "        w.requires_grad_(True)\n",
    "\n",
    "\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    if not num_iters:\n",
    "        num_iters = len(dataloader_iter)\n",
    "        \n",
    "    for it in range(num_iters):\n",
    "        print(\"(1): Iterations %d/%d.\" % (it, num_iters))\n",
    "        inputs, targets = next(dataloader_iter)\n",
    "        N = inputs.shape[0]\n",
    "        din = copy.deepcopy(inputs)\n",
    "        dtarget = copy.deepcopy(targets)\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        while start < N:\n",
    "            end = min(start+intv, N)\n",
    "            print('(1):  %d -> %d.' % (start, end))\n",
    "            inputs_one.append(din[start:end])\n",
    "            targets_one.append(dtarget[start:end])\n",
    "            \n",
    "            try:\n",
    "                outputs = net.forward(inputs[start:end].to(device)) / 200  # divide by temperature to make it uniform\n",
    "                if target_node == 'loss':\n",
    "                    targets = max_likelihood_for_no_target(targets,output) \n",
    "                    loss = criterion(outputs, targets[start:end].to(device))    #running backward pass calls all the hooks and calculates the ranks of all edges and nodes in the graph \n",
    "            except TargetReached:\n",
    "                print('target node %s reached, halted forward pass'%str(target_node))\n",
    "                print(target_node_layer_name)\n",
    "                loss = get_optim_target_from_model(net,target_node_layer_name)\n",
    "\n",
    "\n",
    "            grad_w_p = autograd.grad(loss, weights, create_graph=False)\n",
    "            \n",
    "            if grad_w is None:\n",
    "                grad_w = list(grad_w_p)\n",
    "            else:\n",
    "                for idx in range(len(grad_w)):\n",
    "                    grad_w[idx] += grad_w_p[idx]\n",
    "            start = end\n",
    "\n",
    "    for it in range(len(inputs_one)):\n",
    "        print(\"(2): Iterations %d/%d.\" % (it, len(inputs_one)))\n",
    "        inputs = inputs_one.pop(0).to(device)\n",
    "        targets = targets_one.pop(0).to(device)\n",
    "        \n",
    "        try:\n",
    "            outputs = net.forward(inputs) / 200  # divide by temperature to make it uniform\n",
    "            if target_node == 'loss':\n",
    "                loss = criterion(outputs, targets)    #running backward pass calls all the hooks and calculates the ranks of all edges and nodes in the graph \n",
    "        except TargetReached:\n",
    "            print('target node %s reached, halted forward pass'%str(target_node))\n",
    "            loss = get_optim_target_from_model(net,target_node_layer_name)\n",
    "\n",
    "\n",
    "        grad_f = autograd.grad(loss, weights, create_graph=True)\n",
    "        z = 0\n",
    "        count = 0\n",
    "        for layer in net.modules():\n",
    "            if isinstance(layer, hooked_Conv2d) or isinstance(layer, nn.Linear):\n",
    "                z += (grad_w[count] * grad_f[count]).sum()\n",
    "                if layer.name == target_node_layer_name:\n",
    "                    break\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "        z.backward()\n",
    "\n",
    "    grads = dict()\n",
    "    old_modules = list(old_net.modules())\n",
    "    for idx, layer in enumerate(net.modules()):\n",
    "        if isinstance(layer, hooked_Conv2d) or isinstance(layer, nn.Linear):\n",
    "            grads[layer.name] = -layer.from_conv.weight.data * layer.from_conv.weight.grad  # -theta_q Hg\n",
    "            if layer.name == target_node_layer_name:\n",
    "                break\n",
    "                \n",
    "    all_scores = torch.cat([torch.flatten(x) for x in grads.values()])\n",
    "    norm_factor = torch.abs(torch.sum(all_scores)) + eps\n",
    "    print(\"** grasp norm factor:\", norm_factor)\n",
    "    for layer in grads:\n",
    "        grads[layer] = grads[layer]/norm_factor\n",
    "        grads[layer] = grads[layer].to('cpu')\n",
    "    return grads\n",
    "\n",
    "    '''\n",
    "    old_modules = list(old_net.modules())\n",
    "    for idx, layer in enumerate(net.modules()):\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            grads[old_modules[idx]] = -layer.weight.data * layer.weight.grad  # -theta_q Hg\n",
    "    \n",
    "    # Gather all scores in a single vector and normalise\n",
    "    all_scores = torch.cat([torch.flatten(x) for x in grads.values()])\n",
    "    norm_factor = torch.abs(torch.sum(all_scores)) + eps\n",
    "    print(\"** norm factor:\", norm_factor)\n",
    "    all_scores.div_(norm_factor)\n",
    "\n",
    "    num_params_to_rm = int(len(all_scores) * (1 - keep_ratio))\n",
    "    threshold, _ = torch.topk(all_scores, num_params_to_rm, sorted=True)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    acceptable_score = threshold[-1]\n",
    "    print('** accept: ', acceptable_score)\n",
    "    keep_masks = dict()\n",
    "    for m, g in grads.items():\n",
    "        keep_masks[m] = ((g / norm_factor) <= acceptable_score).float()\n",
    "\n",
    "    print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks.values()])))\n",
    "\n",
    "    return keep_masks\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting scores for target features_10:104\n",
      "(1): Iterations 0/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 1/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 2/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 3/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 4/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 5/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 6/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1): Iterations 7/8.\n",
      "(1):  0 -> 20.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  20 -> 40.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  40 -> 60.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  60 -> 80.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  80 -> 100.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  100 -> 120.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  120 -> 140.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  140 -> 160.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  160 -> 180.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  180 -> 200.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  200 -> 220.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  220 -> 240.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(1):  240 -> 250.\n",
      "target node 1000 reached, halted forward pass\n",
      "features_10\n",
      "(2): Iterations 0/104.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 1/103.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 2/102.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 3/101.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 4/100.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 5/99.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 6/98.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 7/97.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 8/96.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 9/95.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 10/94.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 11/93.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 12/92.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 13/91.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 14/90.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 15/89.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 16/88.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 17/87.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 18/86.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 19/85.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 20/84.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 21/83.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 22/82.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 23/81.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 24/80.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 25/79.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 26/78.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 27/77.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 28/76.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 29/75.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 30/74.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 31/73.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 32/72.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 33/71.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 34/70.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 35/69.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 36/68.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 37/67.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 38/66.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 39/65.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 40/64.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 41/63.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 42/62.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 43/61.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 44/60.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 45/59.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 46/58.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 47/57.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 48/56.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 49/55.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 50/54.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 51/53.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 52/52.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 53/51.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 54/50.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 55/49.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 56/48.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 57/47.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 58/46.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 59/45.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 60/44.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 61/43.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 62/42.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 63/41.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 64/40.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 65/39.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 66/38.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 67/37.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 68/36.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 69/35.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 70/34.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 71/33.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 72/32.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 73/31.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 74/30.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 75/29.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 76/28.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 77/27.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 78/26.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 79/25.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 80/24.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 81/23.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 82/22.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 83/21.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 84/20.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 85/19.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 86/18.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 87/17.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 88/16.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 89/15.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 90/14.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 91/13.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 92/12.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 93/11.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 94/10.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 95/9.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 96/8.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 97/7.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 98/6.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 99/5.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 100/4.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 101/3.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 102/2.\n",
      "target node 1000 reached, halted forward pass\n",
      "(2): Iterations 103/1.\n",
      "target node 1000 reached, halted forward pass\n",
      "** grasp norm factor: tensor(1.0381e+08, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "grasp_scores = feature_GraSP(model,1000 ,.9, image_loader, params['device'], params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keep_masks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-85b8a4fbc079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmean_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keep_masks' is not defined"
     ]
    }
   ],
   "source": [
    "torch.mean(keep_masks['features_0'],dim=(1,2,3)).shape\n",
    "\n",
    "\n",
    "mean_masks = {}\n",
    "max_masks = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "\n",
    "node_columns = ['node_num','layer_name','layer','node_num_by_layer','graspmean_rank']\n",
    "edge_columns = ['edge_num','layer_name','layer','out_channel','in_channel','graspmean_rank','graspmin_rank']\n",
    "\n",
    "node_biglist = []\n",
    "edge_biglist = []\n",
    "\n",
    "edge_color_dict = {0:'r',\n",
    "                   1:'g',\n",
    "                   2:'b'}\n",
    "\n",
    "node_num = 0\n",
    "layer = 0\n",
    "for layer_name in grasp_scores:\n",
    "    print(layer_name)\n",
    "    for node_num_by_layer in range(grasp_scores[layer_name].shape[0]):\n",
    "        node_mean = float(torch.mean(grasp_scores[layer_name][node_num_by_layer,:,:,:]))\n",
    "        node_biglist.append([node_num,layer_name,layer,node_num_by_layer,node_mean])\n",
    "        for in_channel in range(grasp_scores[layer_name].shape[1]):\n",
    "            edge_mean = float(torch.mean(grasp_scores[layer_name][node_num_by_layer,in_channel,:,:]))\n",
    "            edge_min = float(torch.min(grasp_scores[layer_name][node_num_by_layer,in_channel,:,:]))\n",
    "            edge_out = str(params['layer_nodes'][layer][1][node_num_by_layer])\n",
    "            if layer == 0:\n",
    "                edge_in = edge_color_dict[in_channel]\n",
    "            else:\n",
    "                edge_in = str(params['layer_nodes'][layer-1][1][in_channel])\n",
    "            edge_name = edge_in+'-'+edge_out\n",
    "            edge_biglist.append([edge_name,layer_name,layer,node_num_by_layer,in_channel,edge_mean,edge_min])\n",
    "        node_num +=1\n",
    "    layer +=1\n",
    " \n",
    "node_df = pd.DataFrame(node_biglist,columns=node_columns)\n",
    "edge_df = pd.DataFrame(edge_biglist,columns = edge_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(([[1.,2.,3.],[4.,5.,6.]],\n",
    "                 [[7.,8.,9.],[10.,11.,12.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = edge_df # iris is a pandas DataFrame\n",
    "fig = px.violin(df, x=\"layer\", y=\"graspmean_rank\",hover_data=['edge_num'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeletalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skeletal exponential  scheduler \n",
    "\n",
    "from math import log, exp\n",
    "\n",
    "k = 1000 #unpruned parameter target\n",
    "m = 10000 #parameter number before pruning\n",
    "T=10   #pruning steps\n",
    "def k_at_t(t,k=k,T=T,m=m):\n",
    "    return exp(t/T*log(k)+(1-t/T)*log(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import types\n",
    "\n",
    "from math import log, exp, ceil\n",
    "\n",
    "\n",
    "def snip_forward_conv2d(self, x):\n",
    "        return F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
    "                        self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def snip_forward_linear(self, x):\n",
    "        return F.linear(x, self.weight * self.weight_mask, self.bias)\n",
    "\n",
    "\n",
    "def SNIP(net, dataloader, target_node='loss', full_dataset = True, keep_ratio=.1, num_params_to_keep=None, device=None, structure='weights', mask=None):\n",
    "    '''\n",
    "    if num_params_to_keep is specified, this argument overrides keep_ratio\n",
    "    '''\n",
    "\n",
    "    assert structure in ('weights','kernels','filters')    \n",
    "    \n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    # Monkey-patch the Linear and Conv2d layer to learn the multiplicative mask\n",
    "    # instead of the weights\n",
    "    count = 0\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if mask is not None and count < len(mask): #we have a mask for these weights \n",
    "                layer.weight_mask = nn.Parameter(mask[count])\n",
    "            else:\n",
    "                layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "            #nn.init.xavier_normal_(layer.weight)\n",
    "            layer.weight.requires_grad = False\n",
    "            count += 1\n",
    "\n",
    "        # Override the forward methods:\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            layer.forward = types.MethodType(snip_forward_conv2d, layer)\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.forward = types.MethodType(snip_forward_linear, layer)\n",
    "    \n",
    "    #do we iterate through the whole dataset or not\n",
    "    iter_dataloader = iter(dataloader)\n",
    "    \n",
    "    iters = 1\n",
    "    if full_dataset:\n",
    "        iters = len(iter_dataloader)\n",
    "    \n",
    "    \n",
    "    grads_abs = [] #computed scores\n",
    "    \n",
    "    for it in range(iters):\n",
    "        # Grab a single batch from the training dataset\n",
    "        inputs, targets = next(iter_dataloader)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Compute gradients (but don't apply them)\n",
    "        net.zero_grad()\n",
    "        outputs = net.forward(inputs)\n",
    "        loss = F.nll_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        #get weight-wise scores\n",
    "        if grads_abs == []:\n",
    "            for layer in net.modules():\n",
    "                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                    grads_abs.append(torch.abs(layer.weight_mask.grad))\n",
    "        else:\n",
    "            count = 0\n",
    "            for layer in net.modules():\n",
    "                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                    grads_abs[count] += torch.abs(layer.weight_mask.grad)        \n",
    "                    count += 1\n",
    "                \n",
    "                \n",
    "    #structure pruning by weights, kernels, or filters   \n",
    "    \n",
    "    if structure == 'weights':\n",
    "        structure_grads_abs = grads_abs\n",
    "    elif structure == 'kernels':\n",
    "        structure_grads_abs = []\n",
    "        for grad in grads_abs:\n",
    "            if len(grad.shape) == 4: #conv2d layer\n",
    "                structure_grads_abs.append(torch.mean(grad,dim = (2,3))) #average across height and width of each kernel\n",
    "    else:\n",
    "        structure_grads_abs = []\n",
    "        for grad in grads_abs:\n",
    "            if len(grad.shape) == 4: #conv2d layer\n",
    "                structure_grads_abs.append(torch.mean(grad,dim = (1,2,3))) #average across channel height and width of each filter\n",
    "        \n",
    "\n",
    "    # Gather all scores in a single vector and normalise\n",
    "    all_scores = torch.cat([torch.flatten(x) for x in structure_grads_abs])\n",
    "    norm_factor = torch.sum(all_scores)\n",
    "    all_scores.div_(norm_factor)\n",
    "    \n",
    "    #get num params to keep\n",
    "    if num_params_to_keep is None:\n",
    "        num_params_to_keep = int(len(all_scores) * keep_ratio)\n",
    "    threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
    "    acceptable_score = threshold[-1]\n",
    "\n",
    "    keep_masks = []\n",
    "\n",
    "    for g in structure_grads_abs:\n",
    "        keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
    "\n",
    "    #print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))\n",
    "\n",
    "    return(keep_masks)\n",
    "\n",
    "    \n",
    "def gradxweight_SNIP(net, keep_ratio, train_dataloader, device):    \n",
    "        # TODO: shuffle?\n",
    "\n",
    "    # Grab a single batch from the training dataset\n",
    "    inputs, targets = next(iter(train_dataloader))\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Let's create a fresh copy of the network so that we're not worried about\n",
    "    # affecting the actual training-phase\n",
    "    net = copy.deepcopy(net)\n",
    "\n",
    "    # Compute gradients (but don't apply them)\n",
    "    net.zero_grad()\n",
    "    outputs = net.forward(inputs)\n",
    "    loss = F.nll_loss(outputs, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    grads_abs = []\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            grads_abs.append(torch.abs(layer.weight.grad*layer.weight.data))\n",
    "\n",
    "    # Gather all scores in a single vector and normalise\n",
    "    all_scores_flat = torch.cat([torch.flatten(x) for x in grads_abs])\n",
    "    norm_factor = torch.sum(all_scores_flat)\n",
    "    for grad in grads_abs:\n",
    "        grad = grad/norm_factor\n",
    "    all_scores_flat.div_(norm_factor)\n",
    "    return grads_abs, all_scores_flat\n",
    "    '''\n",
    "    num_params_to_keep = int(len(all_scores) * keep_ratio)\n",
    "    threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
    "    acceptable_score = threshold[-1]\n",
    "\n",
    "    keep_masks = []\n",
    "    for g in grads_abs:\n",
    "        keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
    "\n",
    "    print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))\n",
    "\n",
    "    return(keep_masks)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    \n",
    "def apply_prune_mask(net, keep_masks):\n",
    "\n",
    "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
    "    # one-to-one by removing all the irrelevant modules:\n",
    "    prunable_layers = filter(\n",
    "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
    "            layer, nn.Linear), net.modules())\n",
    "\n",
    "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
    "        assert (layer.weight.shape == keep_mask.shape)\n",
    "\n",
    "        def hook_factory(keep_mask):\n",
    "            \"\"\"\n",
    "            The hook function can't be defined directly here because of Python's\n",
    "            late binding which would result in all hooks getting the very last\n",
    "            mask! Getting it through another function forces early binding.\n",
    "            \"\"\"\n",
    "\n",
    "            def hook(grads):\n",
    "                return grads * keep_mask\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # mask[i] == 0 --> Prune parameter\n",
    "        # mask[i] == 1 --> Keep parameter\n",
    "\n",
    "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
    "        # Step 2: Make sure their gradients remain zero\n",
    "        layer.weight.data[keep_mask == 0.] = 0.\n",
    "        layer.weight.register_hook(hook_factory(keep_mask))\n",
    "        \n",
    "        \n",
    "def expand_structured_mask(mask,net):\n",
    "    '''Structured mask might have shape (filter, channel) for kernel structured mask, but the weights have\n",
    "        shape (filter,channel,height,width), so we make a new weight wise mask based on the structured mask'''\n",
    "\n",
    "    weight_mask = []\n",
    "    count=0\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if count < len(mask):\n",
    "                weight_mask.append(mask[count])\n",
    "                while len(weight_mask[-1].shape) < 4:\n",
    "                    weight_mask[-1] = weight_mask[-1].unsqueeze(dim=-1)\n",
    "                weight_mask[-1] = weight_mask[-1].expand(layer.weight.shape)\n",
    "            count+= 1\n",
    "    return weight_mask\n",
    "\n",
    "\n",
    "def force_pruning(net, dataloader, T=10,full_dataset = True, keep_ratio=.1, num_params_to_keep=None, device=None, structure='weights', mask=None):    #progressive skeletonization\n",
    "\n",
    "    \n",
    "    assert structure in ('weights','kernels','filters')\n",
    "    \n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    \n",
    "    total_params = 0\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if structure == 'weights' and (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)):\n",
    "            total_params += len(layer.weight.flatten())\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            if structure == 'kernels':\n",
    "                total_params += int(layer.weight.shape[0]*layer.weight.shape[1])\n",
    "            else:\n",
    "                total_params += int(layer.weight.shape[0])\n",
    "    \n",
    "    if num_params_to_keep is None:\n",
    "        num_params_to_keep = ceil(keep_ratio*total_params)\n",
    "    else:\n",
    "        keep_ratio = num_params_to_keep/total_params       #num_params_to_keep arg overrides keep_ratio\n",
    "    \n",
    "    print('pruning %s'%structure)\n",
    "    print('total parameters: %s'%str(total_params))\n",
    "    print('parameters after pruning: %s'%str(num_params_to_keep))\n",
    "    print('keep ratio: %s'%str(keep_ratio))\n",
    "  \n",
    "\n",
    "    print(\"Pruning with %s pruning steps\"%str(T))\n",
    "    for t in range(1,T+1):\n",
    "        print('step %s'%str(t))\n",
    "        \n",
    "        k = ceil(exp(t/T*log(num_params_to_keep)+(1-t/T)*log(total_params))) #exponential schedulr\n",
    "         \n",
    "        print('%s params'%str(k))\n",
    "        \n",
    "        #SNIP\n",
    "        struct_mask = SNIP(net, dataloader, num_params_to_keep=k, structure=structure, mask=mask, full_dataset = full_dataset, device=device)\n",
    "        if structure is not 'weights':\n",
    "            mask = expand_structured_mask(struct_mask,net) #this weight mask will get applied to the network on the next iteration\n",
    "        \n",
    "    return struct_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA LOADER###\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "from viscnn.data_loading import rank_image_data\n",
    "from viscnn.dissected_Conv2d import *\n",
    "from copy import deepcopy\n",
    "\n",
    "kwargs = {'num_workers': params['num_workers'], 'pin_memory': True, 'sampler':None} if 'cuda' in params['device'] else {}\n",
    "\n",
    "\n",
    "image_loader = data.DataLoader(rank_image_data('/mnt/data/chris/dropbox/Research-Hamblin/Projects/cnn_subgraph_visualizer/image_data/alexnet10/ranking_images',\n",
    "                                                params['preprocess'],\n",
    "                                                label_file_path = '/mnt/data/chris/dropbox/Research-Hamblin/Projects/cnn_subgraph_visualizer/image_data/imagenet_50/labels.txt',class_folders=True),\n",
    "                                                batch_size=250,\n",
    "                                                shuffle=False,\n",
    "                                                **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning kernels\n",
      "total parameters: 250048\n",
      "parameters after pruning: 25005\n",
      "keep ratio: 0.1\n",
      "Pruning with 10 pruning steps\n",
      "step 1\n",
      "198621 params\n",
      "step 2\n",
      "157770 params\n",
      "step 3\n",
      "125322 params\n",
      "step 4\n",
      "99547 params\n",
      "step 5\n",
      "79073 params\n",
      "step 6\n",
      "62810 params\n",
      "step 7\n",
      "49892 params\n",
      "step 8\n",
      "39631 params\n",
      "step 9\n",
      "31480 params\n",
      "step 10\n",
      "25005 params\n"
     ]
    }
   ],
   "source": [
    "mask = force_pruning(model, image_loader,structure='kernels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7917, device='cuda:0')\n",
      "tensor(0.2240, device='cuda:0')\n",
      "tensor(0.1127, device='cuda:0')\n",
      "tensor(0.0870, device='cuda:0')\n",
      "tensor(0.0798, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_layer_ratio(mask):\n",
    "    ones = mask.sum()\n",
    "    total = len(mask.flatten())\n",
    "    return ones/total\n",
    "\n",
    "for i in range(len(mask)):\n",
    "    print(get_layer_ratio(mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999.9999999999998"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log, exp\n",
    "k_at_t(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 11, 11])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mask = []\n",
    "count=0\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "        weights = layer.weight\n",
    "        break\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = mask[0].unsqueeze(dim=-1).unsqueeze(dim=-1).unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 1, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_mask = mask0.expand(mask0.shape[0],weights.shape[1],weights.shape[2],weights.shape[3])\n",
    "ex_mask = mask0.expand(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23232"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex_mask.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gradxweight_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_scores = []\n",
    "for grad in gradxweight_scores:\n",
    "    if len(grad.shape) == 4: #conv2d layer\n",
    "        kernel_scores.append(torch.mean(grad,dim = (2,3)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "print(kernel_scores[0].shape)\n",
    "print(gradxweight_scores[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.mean(gradxweight_scores[0][1,2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006, device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_scores[0][1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.3468e-07, 7.2067e-07, 6.9043e-07,  ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snip_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-de12ddf23dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnip_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradxweight_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/viscnn/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3906\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x and y must have length at least 2.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3908\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3909\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/viscnn/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/viscnn/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(snip_scores,gradxweight_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Force Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def show_model_layer_names(model, getLayerRepr=False):\n",
    "    \"\"\"\n",
    "    If getLayerRepr is True, return a OrderedDict of layer names, layer representation string pair.\n",
    "    If it's False, just return a list of layer names\n",
    "    \"\"\"\n",
    "    \n",
    "    layers = OrderedDict() if getLayerRepr else []\n",
    "    conv_linear_layers = []\n",
    "    # recursive function to get layers\n",
    "    def get_layers(net, prefix=[]):\n",
    "        \n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "\n",
    "                if layer is None:\n",
    "                    # e.g. GoogLeNet's aux1 and aux2 layers\n",
    "                    continue\n",
    "                if getLayerRepr:\n",
    "                    layers[\"_\".join(prefix+[name])] = layer.__repr__()\n",
    "                else:\n",
    "                    layers.append(\"_\".join(prefix + [name]))\n",
    "                \n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    conv_linear_layers.append((\"_\".join(prefix + [name]),'  conv'))\n",
    "                elif isinstance(layer, nn.Linear):\n",
    "                    conv_linear_layers.append((\"_\".join(prefix + [name]),'  linear'))\n",
    "                    \n",
    "                get_layers(layer, prefix=prefix+[name])\n",
    "                \n",
    "    get_layers(model)\n",
    "    \n",
    "    print('All Layers:\\n')\n",
    "    for layer in layers:\n",
    "        print(layer)\n",
    "\n",
    "    print('\\nConvolutional and Linear layers:\\n')\n",
    "    for layer in conv_linear_layers:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import types\n",
    "\n",
    "from math import log, exp, ceil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ref_name_modules(net):\n",
    "    \n",
    "    # recursive function to get layers\n",
    "    def name_layers(net, prefix=[]):\n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "\n",
    "                if layer is None:\n",
    "                    # e.g. GoogLeNet's aux1 and aux2 layers\n",
    "                    continue\n",
    "                \n",
    "                layer.ref_name = \"_\".join(prefix + [name])\n",
    "\n",
    "    name_layers(model)\n",
    "\n",
    "    \n",
    " \n",
    "def feature_targets_list_2_dict(feature_targets,feature_targets_coefficients=None):\n",
    "    if isinstance(feature_targets_coefficients,list):\n",
    "        feature_targets_coefficients_ls = feature_targets_coefficients\n",
    "        feature_targets_coefficients = {}\n",
    "    \n",
    "    #get dict version of feature targets\n",
    "    if isinstance(feature_targets,list):\n",
    "        feature_targets_ls = feature_targets\n",
    "        feature_targets = {}\n",
    "        for i,feature_conj in enumerate(feature_targets_ls):\n",
    "            layer, feature = feature_conj.split(':')\n",
    "            if layer.strip() in feature_targets:\n",
    "                feature_targets[layer.strip()].append(int(feature.strip()))\n",
    "            else:\n",
    "                feature_targets[layer.strip()] = [int(feature.strip())]\n",
    "            if feature_targets_coefficients is not None:\n",
    "                if layer.strip() in feature_targets_coefficients:\n",
    "                    feature_targets_coefficients[layer.strip()].append(feature_targets_coefficients_ls[i])\n",
    "                else:\n",
    "                    feature_targets_coefficients[layer.strip()] = [feature_targets_coefficients_ls[i]]\n",
    "                \n",
    "    assert isinstance(feature_targets,dict)\n",
    "    \n",
    "    if feature_targets_coefficients is None:\n",
    "        return feature_targets\n",
    "    else:\n",
    "        return feature_targets, feature_targets_coefficients\n",
    "    \n",
    "\n",
    "\n",
    "def setup_net_for_circuit_prune(net, feature_targets=None,rank_field = 'image'):\n",
    "    \n",
    "    assert rank_field in ('image','min','max')\n",
    "    \n",
    "    #name model modules\n",
    "    ref_name_modules(net)\n",
    "    \n",
    "    #get dict version of feature targets\n",
    "    feature_targets = feature_targets_list_2_dict(feature_targets)\n",
    "            \n",
    "    \n",
    "    def setup_layers(net):\n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "\n",
    "                if layer is None:\n",
    "                    # e.g. GoogLeNet's aux1 and aux2 layers\n",
    "                    continue\n",
    "\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    layer.rank_field = rank_field\n",
    "                    layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "                    \n",
    "                    layer.feature_target_indices = None\n",
    "                    if feature_targets is not None:\n",
    "                        if layer.ref_name in feature_targets: #layer has feature targets in it\n",
    "                            layer.feature_target_indices = feature_targets[layer.ref_name]\n",
    "\n",
    "                    #setup masked forward pass\n",
    "                    layer.forward = types.MethodType(circuit_prune_forward_conv2d, layer)\n",
    "\n",
    "                elif isinstance(layer, nn.Linear):\n",
    "                    layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "                    \n",
    "                    layer.feature_target_indices = None\n",
    "                    if feature_targets is not None:\n",
    "                        if layer.ref_name in feature_targets: #layer has feature targets in it\n",
    "                            layer.feature_target_indices = feature_targets[layer.ref_name]\n",
    "\n",
    "                    #setup masked forward pass\n",
    "                    layer.forward = types.MethodType(circuit_prune_forward_linear, layer)\n",
    "\n",
    "\n",
    "                setup_layers(layer)\n",
    "\n",
    "           \n",
    "    setup_layers(net)\n",
    "    \n",
    "    \n",
    "    \n",
    "def circuit_prune_forward_conv2d(self, x):\n",
    "        \n",
    "    #pass input through conv and weight mask\n",
    "    x = F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
    "                    self.stride, self.padding, self.dilation, self.groups) \n",
    "\n",
    "    #gather feature targets\n",
    "    if self.feature_target_indices is not None: #there are feature targets in the conv \n",
    "        self.feature_targets = {}\n",
    "        for feature_idx in self.feature_target_indices:\n",
    "            if self.rank_field == 'image':\n",
    "                avg_activations = x.mean(dim=(0, 2, 3))\n",
    "                self.feature_targets[feature_idx] = avg_activations[feature_idx]\n",
    "                \n",
    "            elif self.rank_field == 'max':\n",
    "                max_acts = x.view(x.size(0),x.size(1), x.size(2)*x.size(3)).max(dim=-1).values\n",
    "                max_acts_target = max_acts[:,feature_idx]\n",
    "                self.feature_targets[feature_idx] = max_acts_target.mean()\n",
    "                \n",
    "            elif self.rank_field == 'min':\n",
    "                min_acts = x.view(x.size(0),x.size(1), x.size(2)*x.size(3)).min(dim=-1).values\n",
    "                min_acts_target = min_acts[:,feature_idx]\n",
    "                self.feature_targets[feature_idx] = min_acts_target.mean()\n",
    "                \n",
    "            elif isinstance(self.rank_field,list):\n",
    "                raise Exception('List type rank field not yet implemented, use \"min\", \"max\",or \"image\" as the rank field')\n",
    "                #target_acts = \n",
    "                #optim_target = target_acts.mean()\n",
    "            #print(optim_target)\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def circuit_prune_forward_linear(self, x):\n",
    "    \n",
    "    #pass input through weights and weight mask\n",
    "    x = F.linear(x, self.weight * self.weight_mask, self.bias)\n",
    "\n",
    "    #gather feature targets\n",
    "    if self.feature_target_indices is not None: #there are feature targets in the conv \n",
    "        self.feature_targets = {}\n",
    "        for feature_idx in self.feature_target_indices:\n",
    "            avg_activations = x.mean(dim=(0))\n",
    "            self.feature_targets[feature_idx] = avg_activations[feature_idx] \n",
    "    \n",
    "    return x\n",
    "  \n",
    " \n",
    "def get_feature_target_from_net(net, feature_targets, feature_targets_coefficients,device=None):\n",
    "    \n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    if feature_targets_coefficients is None:\n",
    "        feature_targets = feature_targets_list_2_dict(feature_targets)\n",
    "    else:\n",
    "        feature_targets_indices,feature_targets_coefficients = feature_targets_list_2_dict(feature_targets,feature_targets_coefficients=feature_targets_coefficients) \n",
    "    \n",
    "    feature_targets_values = {}\n",
    "    \n",
    "    def fetch_targets_values(net):\n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "\n",
    "                if layer is None:\n",
    "                    # e.g. GoogLeNet's aux1 and aux2 layers\n",
    "                    continue\n",
    "\n",
    "                if layer.ref_name in feature_targets_indices.keys():\n",
    "                    feature_targets_values[layer.ref_name] = []\n",
    "                    for idx in layer.feature_targets:\n",
    "                        feature_targets_values[layer.ref_name].append(layer.feature_targets[idx])\n",
    "                        \n",
    "                fetch_targets_values(layer)\n",
    "\n",
    "    fetch_targets_values(net)\n",
    "    \n",
    "    target = None\n",
    "    for layer in feature_targets_values:\n",
    "        for idx in range(len(feature_targets_values[layer])):\n",
    "            coeff = 1\n",
    "            if feature_targets_coefficients is not None:\n",
    "                coeff = feature_targets_coefficients[layer][idx] \n",
    "            \n",
    "            if target is None:\n",
    "                target = coeff*feature_targets_values[layer][idx]\n",
    "            else:\n",
    "                target += coeff*feature_targets_values[layer][idx]\n",
    "\n",
    "    print(feature_targets_values)\n",
    "    print(feature_targets_coefficients)\n",
    "    print(target)\n",
    "    return target\n",
    "\n",
    "\n",
    "\n",
    "def clear_feature_targets_from_net(net):\n",
    "\n",
    "    \n",
    "    def clear_layers(net):\n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "\n",
    "                if layer is None:\n",
    "                    # e.g. GoogLeNet's aux1 and aux2 layers\n",
    "                    continue\n",
    "\n",
    "                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                    layer.feature_targets = None\n",
    "\n",
    "\n",
    "                clear_layers(layer)\n",
    "\n",
    "           \n",
    "    clear_layers(net)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_SNIP(net, dataloader, feature_targets = None, feature_targets_coefficients = None, full_dataset = True, keep_ratio=.1, num_params_to_keep=None, device=None, structure='weights', mask=None, criterion= None):\n",
    "    '''\n",
    "    if num_params_to_keep is specified, this argument overrides keep_ratio\n",
    "    '''\n",
    "\n",
    "    assert structure in ('weights','kernels','filters')    \n",
    "    \n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  \n",
    "\n",
    "    #set up cirterion\n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    #apply current mask\n",
    "    count = 0\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if mask is not None and count < len(mask): #we have a mask for these weights \n",
    "                layer.weight_mask = nn.Parameter(mask[count])\n",
    "            else:\n",
    "                layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "            #nn.init.xavier_normal_(layer.weight)\n",
    "            layer.weight.requires_grad = False\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    \n",
    "    #do we iterate through the whole dataset or not\n",
    "    iter_dataloader = iter(dataloader)\n",
    "    \n",
    "    iters = 1\n",
    "    if full_dataset:\n",
    "        iters = len(iter_dataloader)\n",
    "    \n",
    "    \n",
    "    grads_abs = [] #computed scores\n",
    "    \n",
    "    for it in range(iters):\n",
    "        clear_feature_targets_from_net(net)\n",
    "        \n",
    "        # Grab a single batch from the training dataset\n",
    "        inputs, targets = next(iter_dataloader)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Compute gradients (but don't apply them)\n",
    "        net.zero_grad()\n",
    "        outputs = net.forward(inputs)\n",
    "        if feature_targets is None:\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:   #the real target is feature values in the network\n",
    "            loss = get_feature_target_from_net(net, feature_targets, feature_targets_coefficients,device=device)\n",
    "        loss.backward()\n",
    "\n",
    "        #get weight-wise scores\n",
    "        if grads_abs == []:\n",
    "            for layer in net.modules():\n",
    "                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                    print(layer.ref_name)\n",
    "                    grads_abs.append(torch.abs(layer.weight_mask.grad))\n",
    "        else:\n",
    "            count = 0\n",
    "            for layer in net.modules():\n",
    "                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                    grads_abs[count] += torch.abs(layer.weight_mask.grad)        \n",
    "                    count += 1\n",
    "                \n",
    "                \n",
    "    #structure pruning by weights, kernels, or filters   \n",
    "    \n",
    "    if structure == 'weights':\n",
    "        structure_grads_abs = grads_abs\n",
    "    elif structure == 'kernels':\n",
    "        structure_grads_abs = []\n",
    "        for grad in grads_abs:\n",
    "            if len(grad.shape) == 4: #conv2d layer\n",
    "                structure_grads_abs.append(torch.mean(grad,dim = (2,3))) #average across height and width of each kernel\n",
    "    else:\n",
    "        structure_grads_abs = []\n",
    "        for grad in grads_abs:\n",
    "            if len(grad.shape) == 4: #conv2d layer\n",
    "                structure_grads_abs.append(torch.mean(grad,dim = (1,2,3))) #average across channel height and width of each filter\n",
    "        \n",
    "\n",
    "    # Gather all scores in a single vector and normalise\n",
    "    all_scores = torch.cat([torch.flatten(x) for x in structure_grads_abs])\n",
    "    norm_factor = torch.sum(all_scores)\n",
    "    all_scores.div_(norm_factor)\n",
    "    \n",
    "    #get num params to keep\n",
    "    if num_params_to_keep is None:\n",
    "        num_params_to_keep = int(len(all_scores) * keep_ratio)\n",
    "    threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
    "    acceptable_score = threshold[-1]\n",
    "\n",
    "    keep_masks = []\n",
    "\n",
    "    for g in structure_grads_abs:\n",
    "        keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
    "\n",
    "    #print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))\n",
    "\n",
    "    return(keep_masks)\n",
    "\n",
    "    \n",
    "    \n",
    "def apply_prune_mask(net, keep_masks):\n",
    "\n",
    "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
    "    # one-to-one by removing all the irrelevant modules:\n",
    "    prunable_layers = filter(\n",
    "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
    "            layer, nn.Linear), net.modules())\n",
    "\n",
    "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
    "        assert (layer.weight.shape == keep_mask.shape)\n",
    "\n",
    "        def hook_factory(keep_mask):\n",
    "            \"\"\"\n",
    "            The hook function can't be defined directly here because of Python's\n",
    "            late binding which would result in all hooks getting the very last\n",
    "            mask! Getting it through another function forces early binding.\n",
    "            \"\"\"\n",
    "\n",
    "            def hook(grads):\n",
    "                return grads * keep_mask\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # mask[i] == 0 --> Prune parameter\n",
    "        # mask[i] == 1 --> Keep parameter\n",
    "\n",
    "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
    "        # Step 2: Make sure their gradients remain zero\n",
    "        layer.weight.data[keep_mask == 0.] = 0.\n",
    "        layer.weight.register_hook(hook_factory(keep_mask))\n",
    "        \n",
    "        \n",
    "def expand_structured_mask(mask,net):\n",
    "    '''Structured mask might have shape (filter, channel) for kernel structured mask, but the weights have\n",
    "        shape (filter,channel,height,width), so we make a new weight wise mask based on the structured mask'''\n",
    "\n",
    "    weight_mask = []\n",
    "    count=0\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if count < len(mask):\n",
    "                weight_mask.append(mask[count])\n",
    "                while len(weight_mask[-1].shape) < 4:\n",
    "                    weight_mask[-1] = weight_mask[-1].unsqueeze(dim=-1)\n",
    "                weight_mask[-1] = weight_mask[-1].expand(layer.weight.shape)\n",
    "            count+= 1\n",
    "    return weight_mask\n",
    "\n",
    "\n",
    "def circuit_FORCE_pruning(net, dataloader, feature_targets = None,feature_targets_coefficients = None, T=10,full_dataset = True, keep_ratio=.1, num_params_to_keep=None, device=None, structure='weights', rank_field = 'image', mask=None):    #progressive skeletonization\n",
    "\n",
    "    \n",
    "    assert structure in ('weights','kernels','filters')\n",
    "    assert rank_field in ('image','max','min')\n",
    "    \n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net = net.to(device)\n",
    "    \n",
    "    setup_net_for_circuit_prune(net, feature_targets=feature_targets, rank_field = rank_field)\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_params = 0\n",
    "\n",
    "    for layer in net.modules():\n",
    "        if structure == 'weights' and (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)):\n",
    "            total_params += len(layer.weight.flatten())\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            if structure == 'kernels':\n",
    "                total_params += int(layer.weight.shape[0]*layer.weight.shape[1])\n",
    "            else:\n",
    "                total_params += int(layer.weight.shape[0])\n",
    "    \n",
    "    if num_params_to_keep is None:\n",
    "        num_params_to_keep = ceil(keep_ratio*total_params)\n",
    "    else:\n",
    "        keep_ratio = num_params_to_keep/total_params       #num_params_to_keep arg overrides keep_ratio\n",
    "    \n",
    "    print('pruning %s'%structure)\n",
    "    print('total parameters: %s'%str(total_params))\n",
    "    print('parameters after pruning: %s'%str(num_params_to_keep))\n",
    "    print('keep ratio: %s'%str(keep_ratio))\n",
    "  \n",
    "\n",
    "    print(\"Pruning with %s pruning steps\"%str(T))\n",
    "    for t in range(1,T+1):\n",
    "        print('step %s'%str(t))\n",
    "        \n",
    "        k = ceil(exp(t/T*log(num_params_to_keep)+(1-t/T)*log(total_params))) #exponential schedulr\n",
    "         \n",
    "        print('%s params'%str(k))\n",
    "        \n",
    "        #SNIP\n",
    "        struct_mask = circuit_SNIP(net, dataloader, num_params_to_keep=k, feature_targets = feature_targets, feature_targets_coefficients = feature_targets_coefficients, structure=structure, mask=mask, full_dataset = full_dataset, device=device)\n",
    "        if structure is not 'weights':\n",
    "            mask = expand_structured_mask(struct_mask,net) #this weight mask will get applied to the network on the next iteration\n",
    "        \n",
    "    return struct_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Layers:\n",
      "\n",
      "features\n",
      "features_0\n",
      "features_1\n",
      "features_2\n",
      "features_3\n",
      "features_4\n",
      "features_5\n",
      "features_6\n",
      "features_7\n",
      "features_8\n",
      "features_9\n",
      "features_10\n",
      "features_11\n",
      "features_12\n",
      "avgpool\n",
      "classifier\n",
      "classifier_0\n",
      "classifier_1\n",
      "classifier_2\n",
      "classifier_3\n",
      "classifier_4\n",
      "classifier_5\n",
      "classifier_6\n",
      "\n",
      "Convolutional and Linear layers:\n",
      "\n",
      "('features_0', '  conv')\n",
      "('features_3', '  conv')\n",
      "('features_6', '  conv')\n",
      "('features_8', '  conv')\n",
      "('features_10', '  conv')\n",
      "('classifier_1', '  linear')\n",
      "('classifier_4', '  linear')\n",
      "('classifier_6', '  linear')\n"
     ]
    }
   ],
   "source": [
    "show_model_layer_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_net_for_circuit_prune(net, feature_targets=None,rank_field = 'image'):\n",
    "    \n",
    "    assert rank_field in ('image','min','max')\n",
    "    \n",
    "    #name model modules\n",
    "    ref_name_modules(net)\n",
    "    \n",
    "    #get dict version of feature targets\n",
    "    feature_targets = feature_targets_list_2_dict(feature_targets)\n",
    "            \n",
    "    \n",
    "    def setup_layers(net):\n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "\n",
    "                if layer is None:\n",
    "                    # e.g. GoogLeNet's aux1 and aux2 layers\n",
    "                    continue\n",
    "\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    layer.rank_field = rank_field\n",
    "                    layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "                    \n",
    "                    layer.feature_target_indices = None\n",
    "                    if feature_targets is not None:\n",
    "                        if layer.ref_name in feature_targets: #layer has feature targets in it\n",
    "                            layer.feature_target_indices = feature_targets[layer.ref_name]\n",
    "\n",
    "                    #setup masked forward pass\n",
    "                    layer.forward = types.MethodType(circuit_prune_forward_conv2d, layer)\n",
    "\n",
    "                elif isinstance(layer, nn.Linear):\n",
    "                    layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
    "                    \n",
    "                    layer.feature_target_indices = None\n",
    "                    if feature_targets is not None:\n",
    "                        if layer.ref_name in feature_targets: #layer has feature targets in it\n",
    "                            layer.feature_target_indices = feature_targets[layer.ref_name]\n",
    "\n",
    "                    #setup masked forward pass\n",
    "                    layer.forward = types.MethodType(circuit_prune_forward_linear, layer)\n",
    "\n",
    "\n",
    "                setup_layers(layer)\n",
    "\n",
    "           \n",
    "    setup_layers(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_targets = ['features_6:10','features_6:7','features_8:10']\n",
    "feature_targets_coefficients = [2,3,-1]\n",
    "\n",
    "image_loader = data.DataLoader(rank_image_data('/mnt/data/chris/dropbox/Research-Hamblin/Projects/cnn_subgraph_visualizer/image_data/alexnet10/ranking_images',\n",
    "                                                params['preprocess'],\n",
    "                                                label_file_path = '/mnt/data/chris/dropbox/Research-Hamblin/Projects/cnn_subgraph_visualizer/image_data/imagenet_50/labels.txt',class_folders=True),\n",
    "                                                batch_size=250,\n",
    "                                                shuffle=False,\n",
    "                                                **kwargs)\n",
    "\n",
    "\n",
    "from viscnn.model_prep.utils import load_prepped_model_params, load_prepped_model\n",
    "\n",
    "model = load_prepped_model('alexnet_sparse')\n",
    "params = load_prepped_model_params('alexnet_sparse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.0864"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*-0.9243+3*-3.8447+-1*-1.2963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning kernels\n",
      "total parameters: 250048\n",
      "parameters after pruning: 25005\n",
      "keep ratio: 0.1\n",
      "Pruning with 10 pruning steps\n",
      "step 1\n",
      "198621 params\n",
      "{'features_6': [tensor(-0.9243, device='cuda:0', grad_fn=<SelectBackward>), tensor(-3.8447, device='cuda:0', grad_fn=<SelectBackward>)], 'features_8': [tensor(-1.2963, device='cuda:0', grad_fn=<SelectBackward>)]}\n",
      "{'features_6': [2, 3], 'features_8': [-1]}\n",
      "tensor(-12.0865, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "features_0\n",
      "features_3\n",
      "features_6\n",
      "features_8\n",
      "features_10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "abs(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-6df855ed282d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m circuit_force_pruning(model, image_loader, feature_targets = feature_targets,\n\u001b[1;32m      2\u001b[0m                       \u001b[0mfeature_targets_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_targets_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                       keep_ratio=.1, num_params_to_keep=None, device=None, structure='kernels', mask=None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-31c67ad984ae>\u001b[0m in \u001b[0;36mcircuit_force_pruning\u001b[0;34m(net, dataloader, feature_targets, feature_targets_coefficients, T, full_dataset, keep_ratio, num_params_to_keep, device, structure, rank_field, mask)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m#SNIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mstruct_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcircuit_SNIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_params_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_targets_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_targets_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_structured_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this weight mask will get applied to the network on the next iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-31c67ad984ae>\u001b[0m in \u001b[0;36mcircuit_SNIP\u001b[0;34m(net, dataloader, feature_targets, feature_targets_coefficients, full_dataset, keep_ratio, num_params_to_keep, device, structure, mask, criterion)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mgrads_abs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: abs(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "circuit_force_pruning(model, image_loader, feature_targets = feature_targets,\n",
    "                      feature_targets_coefficients = feature_targets_coefficients, T=10,full_dataset = True, \n",
    "                      keep_ratio=.1, num_params_to_keep=None, device=None, structure='kernels', mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viscnn",
   "language": "python",
   "name": "viscnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
