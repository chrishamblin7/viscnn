{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import time\n",
    "import torch\n",
    "from subprocess import call\n",
    "import os\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "from dissected_Conv2d import *\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "os.chdir('../')\n",
    "import prep_model_parameters as params\n",
    "os.chdir('./prep_model_scripts')\n",
    "from torchvision import models\n",
    "\n",
    "import model_classes\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import dissected_Conv2d\n",
    "from dissected_Conv2d import *\n",
    "reload(dissected_Conv2d); from dissected_Conv2d import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = model_classes.cifar_CNN_prunned()\n",
    "cifar10.load_state_dict(torch.load('../models/cifar_prunned_0.816_state_dict.pt'))\n",
    "\n",
    "cifar10_dis = dissect_model(deepcopy(cifar10),cuda=True,big_mem=params.big_mem)\n",
    "cifar10_dis.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "in_t1 = torch.rand([10,3,32,32])\n",
    "in_t2 = torch.rand([10,3,32,32])\n",
    "in_t1.to('cuda')\n",
    "in_t2.to('cuda')\n",
    "\n",
    "target_1 = torch.tensor([1,2,3,4,5,6,7,8,9,0])\n",
    "target_2 = torch.tensor([0,9,8,7,6,5,4,3,2,1])\n",
    "target_1.to('cuda')\n",
    "target_2.to('cuda')\n",
    "\n",
    "target_comb = torch.cat((target_1,target_2),0)\n",
    "target_comb.to('cuda')\n",
    "in_tcomb = torch.cat((in_t1,in_t2),0)\n",
    "in_tcomb.to('cuda')\n",
    "\n",
    "print(in_tcomb.shape)\n",
    "print(target_comb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t1 = cifar10_dis(in_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.criterion(out_t1, Variable(target_1)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_t1 = deepcopy(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t2 = cifar10_dis(in_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.criterion(out_t2, Variable(target_2)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_t1_t2 = deepcopy(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dis.features[0].postbias_ranks_prenorm\n",
    "out_tcomb = cifar10_dis(in_tcomb)\n",
    "params.criterion(out_tcomb, Variable(target_comb)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_tcomb = deepcopy(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranks_t1_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "out_comb = cifar10_dis(in_tcomb)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "params.criterion(out_comb, Variable(target_comb)).backward()\n",
    "print(start-time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex = models.alexnet(pretrained=True)\n",
    "alex.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_dis = dissect_model(deepcopy(alex),cuda=True,big_mem=params.big_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = torch.rand([2,3,224,224])\n",
    "in_tensor = in_tensor.to('cuda')\n",
    "target = torch.tensor([1,2])\n",
    "target = target.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "out_tensor = alex_dis(in_tensor)\n",
    "print(time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "params.criterion(out_tensor, Variable(target)).backward()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test = alex_dis.features[0]\n",
    "conv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_dis.features[7]\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "out_test = conv_test(in_tensor)\n",
    "print(time.time()-start)\n",
    "start = time.time()\n",
    "torch.sum(out_test).backward()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_dis.features[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dis_conv,open('dis_conv_test_6.plk','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "dis_conv.preadd_ranks_prenorm['actxgrad'].shape\n",
    "dis_conv.postbias_ranks_prenorm['actxgrad'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class dissected_Conv2d_edited(torch.nn.Module):       #2d conv Module class that has presum activation maps as intermediate output\n",
    "    \n",
    "    def gen_inout_permutation(self):\n",
    "        '''\n",
    "        When we flatten out all the output channels not to be grouped by 'output channel', we still want the outputs sorted\n",
    "        such that they can be conveniently added based on input channel later\n",
    "        '''\n",
    "        in_chan = self.in_channels\n",
    "        out_chan = self.out_channels\n",
    "        \n",
    "        weight_perm = []\n",
    "        for i in range(in_chan):\n",
    "            for j in range(out_chan):\n",
    "                weight_perm.append(i+j*in_chan)\n",
    "        \n",
    "        add_perm = []\n",
    "        add_indices = {}\n",
    "        for o in range(out_chan):\n",
    "            add_indices[o] = []\n",
    "            for i in range(in_chan):\n",
    "                add_perm.append(o+i*out_chan)\n",
    "                add_indices[o].append(o+i*out_chan)\n",
    "        return torch.LongTensor(weight_perm),torch.LongTensor(add_perm),add_indices\n",
    "\n",
    "\n",
    "    def make_preadd_conv(self):\n",
    "        '''\n",
    "        nn.Conv2d takes in 'in_channel' number of feature maps, and outputs 'out_channel' number of maps. \n",
    "        internally it has in_channel*out_channel number of 2d conv kernels. Normally, featuremaps associated \n",
    "        with a particular output channel resultant from these kernel convolution are all added together,\n",
    "        this function changes a nn.Conv2d module into a module where this final addition doesnt happen. \n",
    "        The final addition can be performed seperately with permute_add_feature_maps.\n",
    "        '''\n",
    "        in_chan = self.in_channels\n",
    "        out_chan = self.out_channels\n",
    "        \n",
    "        kernel_size = self.from_conv.kernel_size\n",
    "        padding = self.from_conv.padding\n",
    "        stride = self.from_conv.stride\n",
    "        new_conv = nn.Conv2d(in_chan,in_chan*out_chan,kernel_size = kernel_size,\n",
    "                             bias = False, padding=padding,stride=stride,groups= in_chan)\n",
    "        new_conv.weight = torch.nn.parameter.Parameter(\n",
    "                self.from_conv.weight.view(in_chan*out_chan,1,kernel_size[0],kernel_size[1])[self.weight_perm])\n",
    "        return new_conv\n",
    "\n",
    "        \n",
    "    def permute_add_featuremaps(self,feature_map):\n",
    "        '''\n",
    "        Perform the sum within output channels step.  (THIS NEEDS TO BE SPEED OPTIMIZED)\n",
    "        '''\n",
    "        x = feature_map\n",
    "        x = x[:, self.add_perm, :, :]\n",
    "        x = torch.split(x.unsqueeze(dim=1),self.in_channels,dim = 2)\n",
    "        x = torch.cat(x,dim = 1)\n",
    "        x = torch.sum(x,dim=2)\n",
    "        return x\n",
    "    \n",
    "    def gen_weight_ranks(self):\n",
    "        weight_ranks_flat = torch.abs(self.preadd_conv.weight).mean(dim=(2,3)).data.squeeze(1)\n",
    "        edge_weight_ranks = []\n",
    "        for o in self.add_indices:\n",
    "            in_chans = []\n",
    "            for i in self.add_indices[o]:\n",
    "                in_chans.append(weight_ranks_flat[i])\n",
    "            edge_weight_ranks.append(in_chans)\n",
    "        edge_weight_ranks = torch.tensor(edge_weight_ranks)\n",
    "        node_weight_ranks = edge_weight_ranks.mean(dim=1)\n",
    "        return weight_ranks_flat, node_weight_ranks\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, from_conv,store_activations=False, store_ranks = False, cuda=True):      # from conv is normal nn.Conv2d object to pull weights and bias from\n",
    "        super(dissected_Conv2d_edited, self).__init__()\n",
    "        self.from_conv = from_conv\n",
    "        self.in_channels = self.from_conv.weight.shape[1]\n",
    "        self.out_channels = self.from_conv.weight.shape[0]\n",
    "        self.cuda = cuda\n",
    "        self.store_activations = store_activations\n",
    "        self.store_ranks = store_ranks\n",
    "        self.postbias_ranks_prenorm = {'act':None,'grad':None,'actxgrad':None}\n",
    "        self.preadd_ranks_prenorm = {'act':None,'grad':None,'actxgrad':None}\n",
    "        self.images_seen = 0\n",
    "        self.weight_perm,self.add_perm,self.add_indices = self.gen_inout_permutation()\n",
    "        self.preadd_conv = self.make_preadd_conv()\n",
    "        self.bias = None\n",
    "        if self.from_conv.bias is not None:\n",
    "            self.bias = from_conv.bias.unsqueeze(1).unsqueeze(1)\n",
    "        #generate a dict that says which indices should be added together in for 'permute_add_featuremaps'\n",
    "\n",
    "\n",
    "        self.preadd_ranks_prenorm['weight'],self.postbias_ranks_prenorm['weight'] = self.gen_weight_ranks()\n",
    "        if self.store_ranks:\n",
    "            self.preadd_out_hook = None\n",
    "            self.postbias_out_hook = None\n",
    "\n",
    "    def compute_edge_rank(self,grad):\n",
    "        start = time.time()\n",
    "        activation = self.preadd_out\n",
    "        #activation_relu = F.relu(activation)\n",
    "        taylor = activation * grad \n",
    "        rank_key  = {'act':activation,'grad':grad,'actxgrad':taylor}\n",
    "        for key in rank_key:\n",
    "            if self.preadd_ranks_prenorm[key] is None: #initialize at 0\n",
    "                self.preadd_ranks_prenorm[key] = torch.FloatTensor(activation.size(1)).zero_()\n",
    "                if self.cuda:\n",
    "                    self.preadd_ranks_prenorm[key] = self.preadd_ranks_prenorm[key].cuda()\n",
    "            map_mean = rank_key[key].mean(dim=(2, 3)).data\n",
    "            mean_sum = map_mean.sum(dim=0).data      \n",
    "            self.postbias_ranks_prenorm[key] += mean_sum    # we sum up the mean activations over all images, after all batches\n",
    "            #have passed through we will average by the number of images seen with self.average_ranks\n",
    "        print('edge_rank time: %s'%str(time.time() - start))\n",
    "\n",
    "\n",
    "\n",
    "    def compute_node_rank(self,grad):\n",
    "        start = time.time()\n",
    "        activation = self.postbias_out\n",
    "        activation_relu = F.relu(activation)\n",
    "        taylor = activation * grad \n",
    "        rank_key  = {'act':activation_relu,'grad':grad,'actxgrad':taylor}\n",
    "        for key in rank_key:\n",
    "            if self.postbias_ranks_prenorm[key] is None: #initialize at 0\n",
    "                self.postbias_ranks_prenorm[key] = torch.FloatTensor(activation.size(1)).zero_()\n",
    "                if self.cuda:\n",
    "                    self.postbias_ranks_prenorm[key] = self.postbias_ranks_prenorm[key].cuda()\n",
    "            mean = rank_key[key].mean(dim=(0, 2, 3)).data\n",
    "            self.postbias_ranks_prenorm[key] += mean\n",
    "        print('node_rank time: %s'%str(time.time() - start))\n",
    "\n",
    "\n",
    "\n",
    "    def format_edges(self, data= 'activations',prenorm=False):\n",
    "        #fetch preadd activations as [img,out_channel, in_channel,h,w]\n",
    "        #fetch preadd ranks as [out_chan,in_chan]\n",
    "\n",
    "        if not self.store_activations:\n",
    "            print('activations arent stored, use \"store_activations=True\" on model init. returning None')\n",
    "            return None\n",
    "        out_acts_list = []\n",
    "        if data == 'activations':\n",
    "            for out_chan in self.add_indices:\n",
    "                in_acts_list = []\n",
    "                for in_chan in self.add_indices[out_chan]:\n",
    "                    in_acts_list.append(self.preadd_out[:,in_chan,:,:].unsqueeze(dim=1).unsqueeze(dim=1))                    \n",
    "                out_acts_list.append(torch.cat(in_acts_list,dim=2))\n",
    "            return torch.cat(out_acts_list,dim=1).cpu().detach().numpy()\n",
    "\n",
    "        else:\n",
    "            output = {}\n",
    "            for rank_type in ['act','grad','actxgrad','weight']:\n",
    "                out_acts_list = []\n",
    "                for out_chan in self.add_indices:\n",
    "                    in_acts_list = []\n",
    "                    for in_chan in self.add_indices[out_chan]:\n",
    "                        if not prenorm:\n",
    "                            in_acts_list.append(self.preadd_ranks[rank_type][in_chan].unsqueeze(dim=0).unsqueeze(dim=0)) \n",
    "                        else:\n",
    "                            in_acts_list.append(self.preadd_ranks_prenorm[rank_type][in_chan].unsqueeze(dim=0).unsqueeze(dim=0))                 \n",
    "                    out_acts_list.append(torch.cat(in_acts_list,dim=1))\n",
    "                output[rank_type] = torch.cat(out_acts_list,dim=0).cpu().detach().numpy()\n",
    "            return output\n",
    "                        \n",
    "    def average_ranks(self):\n",
    "        for rank_type in ['act','grad','actxgrad']:\n",
    "            self.preadd_ranks_prenorm[rank_type] = self.preadd_ranks_prenorm[rank_type]/self.images_seen\n",
    "            self.postbias_ranks_prenorm[rank_type] = self.postbias_ranks_prenorm[rank_type]/self.images_seen\n",
    "\n",
    "\n",
    "    def normalize_ranks(self):\n",
    "        self.preadd_ranks = {}\n",
    "        self.postbias_ranks = {}\n",
    "        for rank_type in ['act','grad','actxgrad','weight']:\n",
    "            e = torch.abs(self.preadd_ranks_prenorm[rank_type])\n",
    "            n = torch.abs(self.postbias_ranks_prenorm[rank_type])\n",
    "\n",
    "            e = e.cpu()\n",
    "            e = e / np.sqrt(torch.sum(e * e))\n",
    "            n = n.cpu()\n",
    "            n = n / np.sqrt(torch.sum(n * n))\n",
    "\n",
    "            self.preadd_ranks[rank_type] = e\n",
    "            self.postbias_ranks[rank_type] = n\n",
    "\n",
    "        #self.preadd_ranks_prenorm['weight'] = self.preadd_ranks_prenorm['weight'].cpu()\n",
    "        #self.postbias_ranks_prenorm['weight'] = self.postbias_ranks_prenorm['weight'].cpu()\n",
    "        #self.preadd_ranks['weight'] = torch.abs(self.preadd_ranks_prenorm['weight'] )/np.sqrt(torch.sum(self.preadd_ranks_prenorm['weight'] *self.preadd_ranks_prenorm['weight'] ))\n",
    "        #self.postbias_ranks['weight'] = torch.abs(self.postbias_ranks_prenorm['weight'])/np.sqrt(torch.sum(self.postbias_ranks_prenorm['weight']*self.postbias_ranks_prenorm['weight']))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pdb.set_trace()\n",
    "        self.images_seen += x.shape[0]    #keep track of how many images weve seen so we know what to divide by when we average ranks\n",
    "        if self.store_activations:\n",
    "            self.input = x\n",
    "\n",
    "        preadd_out = self.preadd_conv(x)  #get output of convolutions\n",
    "\n",
    "        #store values of intermediate outputs after convolution\n",
    "        if self.store_activations:\n",
    "            self.preadd_out = preadd_out\n",
    " \n",
    "        #Set hooks for calculating rank on backward pass\n",
    "        if self.store_ranks:\n",
    "            self.preadd_out = preadd_out\n",
    "            if self.preadd_out_hook is not None:\n",
    "                self.preadd_out_hook.remove()\n",
    "            self.preadd_out_hook = self.preadd_out.register_hook(self.compute_edge_rank)\n",
    "            #if self.preadd_ranks is not None:\n",
    "            #    print(self.preadd_ranks.shape)\n",
    "\n",
    "        added_out = self.permute_add_featuremaps(preadd_out)    #add convolution outputs by output channel\n",
    "        if self.bias is not None:  \n",
    "            postbias_out = added_out + self.bias\n",
    "        else:\n",
    "            postbias_out = added_out\n",
    "\n",
    "        #Store values of final module output\n",
    "        if self.store_activations:\n",
    "            self.postbias_out = postbias_out\n",
    " \n",
    "        #Set hooks for calculating rank on backward pass\n",
    "        if self.store_ranks:\n",
    "            self.postbias_out = postbias_out\n",
    "            if self.postbias_out_hook is not None:\n",
    "                self.postbias_out_hook.remove()\n",
    "            self.postbias_out_hook = self.postbias_out.register_hook(self.compute_node_rank)\n",
    "            #if self.postbias_ranks is not None:\n",
    "            #    print(self.postbias_ranks.shape)\n",
    "\n",
    "        return postbias_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "in_tensor = torch.rand([2,2,4,4])\n",
    "in_tensor = in_tensor.to('cuda')\n",
    "\n",
    "normal_conv = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, padding=1,bias=False)\n",
    "normal_conv.to('cuda')\n",
    "\n",
    "dis_conv = dissected_Conv2d_edited(normal_conv,cuda=True,store_activations=True,store_ranks=False)\n",
    "dis_conv.to('cuda')\n",
    "\n",
    "flat_conv = nn.Conv2d(2,2*3,kernel_size = 3, \n",
    "                                 bias = False, padding=1,groups= 2)\n",
    "flat_conv.to('cuda')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat\n",
      "0.00045680999755859375\n",
      "0.0013132095336914062\n",
      "normal\n",
      "0.0006017684936523438\n",
      "0.0006968975067138672\n",
      "dissected\n",
      "0.0008349418640136719\n",
      "0.0011136531829833984\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('flat')\n",
    "start = time.time()\n",
    "flat_out = flat_conv(in_tensor)\n",
    "print(time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "torch.sum(flat_out).backward()\n",
    "print(time.time()-start)\n",
    "\n",
    "\n",
    "print('normal')\n",
    "start = time.time()\n",
    "normal_out = normal_conv(in_tensor)\n",
    "normal_hook = normal_out.register_hook(print_grad)\n",
    "print(time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "torch.sum(normal_out).backward()\n",
    "print(time.time()-start)\n",
    "\n",
    "print('dissected')\n",
    "start = time.time()\n",
    "dis_out = dis_conv(in_tensor)\n",
    "dis_hook = dis_out.register_hook(print_grad)\n",
    "print(time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "torch.sum(dis_out).backward()\n",
    "print(time.time()-start)\n",
    "\n",
    "\n",
    "normal_hook.remove()\n",
    "dis_hook.remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dis_conv.preadd_out.shape)\n",
    "dis_conv.postbias_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[10.2030, 12.1269,  8.9663],\n",
      "          [12.7775, 16.0182, 12.1044],\n",
      "          [ 8.4637, 10.6205,  8.0932]],\n",
      "\n",
      "         [[ 9.2588, 11.4218,  9.2084],\n",
      "          [12.4179, 14.9437, 11.5610],\n",
      "          [ 8.6585, 10.9494,  8.8812]]],\n",
      "\n",
      "\n",
      "        [[[10.2030, 12.1269,  8.9663],\n",
      "          [12.7775, 16.0182, 12.1044],\n",
      "          [ 8.4637, 10.6205,  8.0932]],\n",
      "\n",
      "         [[ 9.2588, 11.4218,  9.2084],\n",
      "          [12.4179, 14.9437, 11.5610],\n",
      "          [ 8.6585, 10.9494,  8.8812]]],\n",
      "\n",
      "\n",
      "        [[[10.2030, 12.1269,  8.9663],\n",
      "          [12.7775, 16.0182, 12.1044],\n",
      "          [ 8.4637, 10.6205,  8.0932]],\n",
      "\n",
      "         [[ 9.2588, 11.4218,  9.2084],\n",
      "          [12.4179, 14.9437, 11.5610],\n",
      "          [ 8.6585, 10.9494,  8.8812]]]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0597,  0.1303,  0.0172],\n",
       "          [-0.1065,  0.0494, -0.0644],\n",
       "          [-0.0824, -0.2032,  0.0548]]],\n",
       "\n",
       "\n",
       "        [[[-0.1814,  0.1968,  0.1578],\n",
       "          [ 0.0421, -0.0714, -0.1748],\n",
       "          [-0.0526,  0.0801, -0.1580]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0813,  0.1622, -0.1994],\n",
       "          [-0.1360, -0.1399, -0.0256],\n",
       "          [ 0.2037, -0.1153,  0.1608]]],\n",
       "\n",
       "\n",
       "        [[[-0.1128, -0.0132, -0.0503],\n",
       "          [-0.0769,  0.1416, -0.0356],\n",
       "          [-0.2161, -0.0056,  0.0081]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2290, -0.0238, -0.0569],\n",
       "          [-0.0992,  0.1032, -0.0535],\n",
       "          [ 0.1534, -0.1386, -0.1436]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2076,  0.0046,  0.0629],\n",
       "          [-0.0651,  0.0520,  0.2277],\n",
       "          [-0.0322, -0.1147, -0.0268]]]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(normal_conv.weight.grad)\n",
    "dis_conv.preadd_conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_conv.perm = torch.LongTensor([0,3,1,4,2,5])\n",
    "dis_conv.preadd_perm_out = dis_conv.preadd_out[:, dis_conv.perm, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0835,  0.1532, -0.0085,  0.1581],\n",
      "          [-0.0362,  0.0152, -0.0291,  0.3086],\n",
      "          [ 0.2007,  0.0728,  0.1269,  0.1630],\n",
      "          [ 0.1240, -0.1244, -0.0092,  0.0798]],\n",
      "\n",
      "         [[ 0.0843,  0.1335,  0.1647,  0.2599],\n",
      "          [ 0.0304,  0.2660,  0.3304,  0.3528],\n",
      "          [ 0.2403,  0.3264,  0.1593,  0.2111],\n",
      "          [ 0.1372,  0.1359,  0.1880,  0.3832]],\n",
      "\n",
      "         [[ 0.1834,  0.2307,  0.2740,  0.0208],\n",
      "          [ 0.1551,  0.1430,  0.0237, -0.0393],\n",
      "          [ 0.0555,  0.1059,  0.2327,  0.1310],\n",
      "          [ 0.1885,  0.1629,  0.1833, -0.1638]],\n",
      "\n",
      "         [[-0.1001,  0.1296,  0.0153,  0.1149],\n",
      "          [-0.1324,  0.0354,  0.1181,  0.3244],\n",
      "          [ 0.0825,  0.1823,  0.0357,  0.1931],\n",
      "          [ 0.0230,  0.1334,  0.1731,  0.0542]],\n",
      "\n",
      "         [[ 0.0333,  0.1129,  0.1247,  0.1239],\n",
      "          [ 0.0294,  0.1167,  0.0678,  0.1370],\n",
      "          [ 0.1868,  0.0719,  0.2172,  0.0305],\n",
      "          [ 0.0639,  0.1074,  0.0423,  0.1072]],\n",
      "\n",
      "         [[ 0.0582,  0.0573, -0.0008,  0.1318],\n",
      "          [ 0.1215,  0.2511,  0.2616,  0.2882],\n",
      "          [ 0.2805,  0.3105,  0.3191,  0.3569],\n",
      "          [ 0.1309,  0.2612,  0.2443,  0.2705]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1212,  0.1778,  0.0262,  0.1650],\n",
      "          [ 0.0772,  0.0445,  0.1577,  0.1044],\n",
      "          [ 0.1442, -0.0544,  0.2211,  0.2209],\n",
      "          [ 0.0703, -0.1703,  0.0423,  0.1794]],\n",
      "\n",
      "         [[ 0.1031,  0.2333,  0.2167,  0.0887],\n",
      "          [ 0.0731,  0.3106,  0.2240,  0.1803],\n",
      "          [ 0.2574,  0.2644,  0.3556,  0.3880],\n",
      "          [ 0.1207, -0.0222,  0.2274,  0.3879]],\n",
      "\n",
      "         [[ 0.1491,  0.1275,  0.0640,  0.0511],\n",
      "          [ 0.1168,  0.1756,  0.1085,  0.1885],\n",
      "          [ 0.0841,  0.0622,  0.2532, -0.1165],\n",
      "          [ 0.0078,  0.3302,  0.0864,  0.0433]],\n",
      "\n",
      "         [[-0.0804, -0.1631, -0.0914,  0.2943],\n",
      "          [ 0.0022,  0.0893, -0.0356,  0.1684],\n",
      "          [-0.1412,  0.1113,  0.2783,  0.2529],\n",
      "          [-0.0605,  0.0640,  0.2428,  0.1586]],\n",
      "\n",
      "         [[ 0.0059, -0.0085,  0.1290, -0.0520],\n",
      "          [ 0.0541,  0.1516,  0.1621,  0.0969],\n",
      "          [ 0.0642,  0.1157,  0.2282,  0.1997],\n",
      "          [ 0.0381,  0.0775,  0.0570,  0.0251]],\n",
      "\n",
      "         [[ 0.0127,  0.0515,  0.2245,  0.1723],\n",
      "          [ 0.0693,  0.1787,  0.2664,  0.3381],\n",
      "          [ 0.0737,  0.1721,  0.2605,  0.3790],\n",
      "          [ 0.0974,  0.2869,  0.3508,  0.2612]]]], device='cuda:0',\n",
      "       grad_fn=<ThnnConvDepthwise2DBackward>)\n",
      "tensor([[[[ 0.0835,  0.1532, -0.0085,  0.1581],\n",
      "          [-0.0362,  0.0152, -0.0291,  0.3086],\n",
      "          [ 0.2007,  0.0728,  0.1269,  0.1630],\n",
      "          [ 0.1240, -0.1244, -0.0092,  0.0798]],\n",
      "\n",
      "         [[-0.1001,  0.1296,  0.0153,  0.1149],\n",
      "          [-0.1324,  0.0354,  0.1181,  0.3244],\n",
      "          [ 0.0825,  0.1823,  0.0357,  0.1931],\n",
      "          [ 0.0230,  0.1334,  0.1731,  0.0542]],\n",
      "\n",
      "         [[ 0.0843,  0.1335,  0.1647,  0.2599],\n",
      "          [ 0.0304,  0.2660,  0.3304,  0.3528],\n",
      "          [ 0.2403,  0.3264,  0.1593,  0.2111],\n",
      "          [ 0.1372,  0.1359,  0.1880,  0.3832]],\n",
      "\n",
      "         [[ 0.0333,  0.1129,  0.1247,  0.1239],\n",
      "          [ 0.0294,  0.1167,  0.0678,  0.1370],\n",
      "          [ 0.1868,  0.0719,  0.2172,  0.0305],\n",
      "          [ 0.0639,  0.1074,  0.0423,  0.1072]],\n",
      "\n",
      "         [[ 0.1834,  0.2307,  0.2740,  0.0208],\n",
      "          [ 0.1551,  0.1430,  0.0237, -0.0393],\n",
      "          [ 0.0555,  0.1059,  0.2327,  0.1310],\n",
      "          [ 0.1885,  0.1629,  0.1833, -0.1638]],\n",
      "\n",
      "         [[ 0.0582,  0.0573, -0.0008,  0.1318],\n",
      "          [ 0.1215,  0.2511,  0.2616,  0.2882],\n",
      "          [ 0.2805,  0.3105,  0.3191,  0.3569],\n",
      "          [ 0.1309,  0.2612,  0.2443,  0.2705]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1212,  0.1778,  0.0262,  0.1650],\n",
      "          [ 0.0772,  0.0445,  0.1577,  0.1044],\n",
      "          [ 0.1442, -0.0544,  0.2211,  0.2209],\n",
      "          [ 0.0703, -0.1703,  0.0423,  0.1794]],\n",
      "\n",
      "         [[-0.0804, -0.1631, -0.0914,  0.2943],\n",
      "          [ 0.0022,  0.0893, -0.0356,  0.1684],\n",
      "          [-0.1412,  0.1113,  0.2783,  0.2529],\n",
      "          [-0.0605,  0.0640,  0.2428,  0.1586]],\n",
      "\n",
      "         [[ 0.1031,  0.2333,  0.2167,  0.0887],\n",
      "          [ 0.0731,  0.3106,  0.2240,  0.1803],\n",
      "          [ 0.2574,  0.2644,  0.3556,  0.3880],\n",
      "          [ 0.1207, -0.0222,  0.2274,  0.3879]],\n",
      "\n",
      "         [[ 0.0059, -0.0085,  0.1290, -0.0520],\n",
      "          [ 0.0541,  0.1516,  0.1621,  0.0969],\n",
      "          [ 0.0642,  0.1157,  0.2282,  0.1997],\n",
      "          [ 0.0381,  0.0775,  0.0570,  0.0251]],\n",
      "\n",
      "         [[ 0.1491,  0.1275,  0.0640,  0.0511],\n",
      "          [ 0.1168,  0.1756,  0.1085,  0.1885],\n",
      "          [ 0.0841,  0.0622,  0.2532, -0.1165],\n",
      "          [ 0.0078,  0.3302,  0.0864,  0.0433]],\n",
      "\n",
      "         [[ 0.0127,  0.0515,  0.2245,  0.1723],\n",
      "          [ 0.0693,  0.1787,  0.2664,  0.3381],\n",
      "          [ 0.0737,  0.1721,  0.2605,  0.3790],\n",
      "          [ 0.0974,  0.2869,  0.3508,  0.2612]]]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(dis_conv.preadd_out)\n",
    "\n",
    "print(dis_conv.preadd_perm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_conv.preadd_perm_split_out = torch.split(dis_conv.preadd_perm_out.unsqueeze(dim=1),2,dim = 2)\n",
    "dis_conv.preadd_perm_split_cat_out = torch.cat(dis_conv.preadd_perm_split_out,dim = 1)\n",
    "dis_conv.preadd_perm_split_cat_sum_out = torch.sum(dis_conv.preadd_perm_split_cat_out,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 4, 4])\n",
      "torch.Size([2, 6, 4, 4])\n",
      "torch.Size([2, 3, 2, 4, 4])\n",
      "torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(dis_conv.preadd_perm_split_out[0].shape)\n",
    "print(dis_conv.preadd_out.shape)\n",
    "print(dis_conv.preadd_perm_split_cat_out.shape)\n",
    "print(dis_conv.preadd_perm_split_cat_sum_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0166,  0.2827,  0.0068,  0.2731],\n",
      "          [-0.1686,  0.0506,  0.0890,  0.6331],\n",
      "          [ 0.2832,  0.2551,  0.1625,  0.3561],\n",
      "          [ 0.1471,  0.0090,  0.1639,  0.1339]],\n",
      "\n",
      "         [[ 0.1176,  0.2464,  0.2893,  0.3838],\n",
      "          [ 0.0598,  0.3826,  0.3983,  0.4898],\n",
      "          [ 0.4272,  0.3983,  0.3764,  0.2416],\n",
      "          [ 0.2011,  0.2434,  0.2303,  0.4904]],\n",
      "\n",
      "         [[ 0.2416,  0.2880,  0.2732,  0.1526],\n",
      "          [ 0.2766,  0.3940,  0.2852,  0.2489],\n",
      "          [ 0.3360,  0.4164,  0.5518,  0.4879],\n",
      "          [ 0.3194,  0.4241,  0.4276,  0.1067]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0408,  0.0147, -0.0652,  0.4593],\n",
      "          [ 0.0794,  0.1338,  0.1221,  0.2728],\n",
      "          [ 0.0030,  0.0568,  0.4994,  0.4738],\n",
      "          [ 0.0099, -0.1062,  0.2851,  0.3380]],\n",
      "\n",
      "         [[ 0.1090,  0.2248,  0.3457,  0.0367],\n",
      "          [ 0.1272,  0.4621,  0.3861,  0.2773],\n",
      "          [ 0.3216,  0.3801,  0.5838,  0.5877],\n",
      "          [ 0.1588,  0.0553,  0.2844,  0.4130]],\n",
      "\n",
      "         [[ 0.1617,  0.1790,  0.2885,  0.2234],\n",
      "          [ 0.1861,  0.3544,  0.3749,  0.5266],\n",
      "          [ 0.1578,  0.2343,  0.5137,  0.2625],\n",
      "          [ 0.1051,  0.6170,  0.4372,  0.3045]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnConvolutionBackward>)\n",
      "tensor([[[[-0.0166,  0.2827,  0.0068,  0.2731],\n",
      "          [-0.1686,  0.0506,  0.0890,  0.6331],\n",
      "          [ 0.2832,  0.2551,  0.1625,  0.3561],\n",
      "          [ 0.1471,  0.0090,  0.1639,  0.1339]],\n",
      "\n",
      "         [[ 0.1176,  0.2464,  0.2893,  0.3838],\n",
      "          [ 0.0598,  0.3826,  0.3983,  0.4898],\n",
      "          [ 0.4272,  0.3983,  0.3764,  0.2416],\n",
      "          [ 0.2011,  0.2434,  0.2303,  0.4904]],\n",
      "\n",
      "         [[ 0.2416,  0.2880,  0.2732,  0.1526],\n",
      "          [ 0.2766,  0.3940,  0.2852,  0.2489],\n",
      "          [ 0.3360,  0.4164,  0.5518,  0.4879],\n",
      "          [ 0.3194,  0.4241,  0.4276,  0.1067]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0408,  0.0147, -0.0652,  0.4593],\n",
      "          [ 0.0794,  0.1338,  0.1221,  0.2728],\n",
      "          [ 0.0030,  0.0568,  0.4994,  0.4738],\n",
      "          [ 0.0099, -0.1062,  0.2851,  0.3380]],\n",
      "\n",
      "         [[ 0.1090,  0.2248,  0.3457,  0.0367],\n",
      "          [ 0.1272,  0.4621,  0.3861,  0.2773],\n",
      "          [ 0.3216,  0.3801,  0.5838,  0.5877],\n",
      "          [ 0.1588,  0.0553,  0.2844,  0.4130]],\n",
      "\n",
      "         [[ 0.1617,  0.1790,  0.2885,  0.2234],\n",
      "          [ 0.1861,  0.3544,  0.3749,  0.5266],\n",
      "          [ 0.1578,  0.2343,  0.5137,  0.2625],\n",
      "          [ 0.1051,  0.6170,  0.4372,  0.3045]]]], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(normal_out)\n",
    "print(dis_conv.preadd_perm_split_cat_sum_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3505, -0.4833],\n",
      "        [-0.1438,  1.5109],\n",
      "        [-0.6097, -0.1263],\n",
      "        [ 0.9670,  0.3252]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.1371,  1.2266])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4,2)\n",
    "print(a)\n",
    "torch.sum(a, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning_viz",
   "language": "python",
   "name": "pruning_viz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
