{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import time\n",
    "import torch\n",
    "from subprocess import call\n",
    "import os\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "from dissected_Conv2d import *\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "os.chdir('../')\n",
    "import prep_model_parameters as params\n",
    "os.chdir('./prep_model_scripts')\n",
    "from torchvision import models\n",
    "\n",
    "import model_classes\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import dissected_Conv2d\n",
    "from dissected_Conv2d import *\n",
    "reload(dissected_Conv2d); from dissected_Conv2d import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = model_classes.cifar_CNN_prunned()\n",
    "cifar10.load_state_dict(torch.load('../models/cifar_prunned_0.816_state_dict.pt'))\n",
    "\n",
    "cifar10_dis = dissect_model(deepcopy(cifar10),cuda=True)\n",
    "cifar10_dis.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "in_t1 = torch.rand([10,3,32,32])\n",
    "in_t2 = torch.rand([10,3,32,32])\n",
    "in_t1.to('cuda')\n",
    "in_t2.to('cuda')\n",
    "\n",
    "target_1 = torch.tensor([1,2,3,4,5,6,7,8,9,0])\n",
    "target_2 = torch.tensor([0,9,8,7,6,5,4,3,2,1])\n",
    "target_1.to('cuda')\n",
    "target_2.to('cuda')\n",
    "\n",
    "target_comb = torch.cat((target_1,target_2),0)\n",
    "target_comb.to('cuda')\n",
    "in_tcomb = torch.cat((in_t1,in_t2),0)\n",
    "in_tcomb.to('cuda')\n",
    "\n",
    "in_tcomb = Variable(in_tcomb)\n",
    "\n",
    "print(in_tcomb.shape)\n",
    "print(target_comb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    cifar10_dis.zero_grad()\n",
    "    output = cifar10_dis(in_tcomb)    #running forward pass sets up hooks and stores activations in each dissected_Conv2d module\n",
    "    params.criterion(output, Variable(target_comb)).backward()    #running backward\n",
    "    print(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t1 = cifar10_dis(in_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.criterion(out_t1, Variable(target_1)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_t1 = deepcopy(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_t2 = cifar10_dis(in_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.criterion(out_t2, Variable(target_2)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_t1_t2 = deepcopy(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dis.features[0].postbias_ranks_prenorm\n",
    "out_tcomb = cifar10_dis(in_tcomb)\n",
    "params.criterion(out_tcomb, Variable(target_comb)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_tcomb = deepcopy(cifar10_dis.features[0].preadd_ranks_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranks_t1_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "out_comb = cifar10_dis(in_tcomb)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "params.criterion(out_comb, Variable(target_comb)).backward()\n",
    "print(start-time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex = models.alexnet(pretrained=True)\n",
    "alex.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_dis = dissect_model(deepcopy(alex),cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = torch.rand([2,3,224,224])\n",
    "in_tensor = in_tensor.to('cuda')\n",
    "target = torch.tensor([1,2])\n",
    "target = target.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "out_tensor = alex_dis(in_tensor)\n",
    "print(time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "params.criterion(out_tensor, Variable(target)).backward()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "out_tensor = alex(in_tensor)\n",
    "print(time.time() -start)\n",
    "start = time.time()\n",
    "params.criterion(out_tensor, Variable(target)).backward()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_test = alex_dis.features[0]\n",
    "conv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_dis.features[7]\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "out_test = conv_test(in_tensor)\n",
    "print(time.time()-start)\n",
    "start = time.time()\n",
    "torch.sum(out_test).backward()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_dis.features[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dis_conv,open('dis_conv_test_6.plk','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "dis_conv.preadd_ranks_prenorm['actxgrad'].shape\n",
    "dis_conv.postbias_ranks_prenorm['actxgrad'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class dissected_Conv2d_edit(torch.nn.Module):       #2d conv Module class that has presum activation maps as intermediate output\n",
    "    \n",
    "    def gen_inout_permutation(self):\n",
    "        '''\n",
    "        When we flatten out all the output channels not to be grouped by 'output channel', we still want the outputs sorted\n",
    "        such that they can be conveniently added based on input channel later\n",
    "        '''\n",
    "        in_chan = self.in_channels\n",
    "        out_chan = self.out_channels\n",
    "        \n",
    "        weight_perm = []\n",
    "        for i in range(in_chan):\n",
    "            for j in range(out_chan):\n",
    "                weight_perm.append(i+j*in_chan)\n",
    "        \n",
    "        add_perm = []\n",
    "        add_indices = {}\n",
    "        for o in range(out_chan):\n",
    "            add_indices[o] = []\n",
    "            for i in range(in_chan):\n",
    "                add_perm.append(o+i*out_chan)\n",
    "                add_indices[o].append(o+i*out_chan)\n",
    "        return torch.LongTensor(weight_perm),torch.LongTensor(add_perm),add_indices\n",
    "\n",
    "\n",
    "    def make_preadd_conv(self):\n",
    "        '''\n",
    "        nn.Conv2d takes in 'in_channel' number of feature maps, and outputs 'out_channel' number of maps. \n",
    "        internally it has in_channel*out_channel number of 2d conv kernels. Normally, featuremaps associated \n",
    "        with a particular output channel resultant from these kernel convolution are all added together,\n",
    "        this function changes a nn.Conv2d module into a module where this final addition doesnt happen. \n",
    "        The final addition can be performed seperately with permute_add_feature_maps.\n",
    "        '''\n",
    "        in_chan = self.in_channels\n",
    "        out_chan = self.out_channels\n",
    "        \n",
    "        kernel_size = self.from_conv.kernel_size\n",
    "        padding = self.from_conv.padding\n",
    "        stride = self.from_conv.stride\n",
    "        new_conv = nn.Conv2d(in_chan,in_chan*out_chan,kernel_size = kernel_size,\n",
    "                             bias = False, padding=padding,stride=stride,groups= in_chan)\n",
    "        new_conv.weight = torch.nn.parameter.Parameter(\n",
    "                self.from_conv.weight.view(in_chan*out_chan,1,kernel_size[0],kernel_size[1])[self.weight_perm])\n",
    "        return new_conv\n",
    "\n",
    "        \n",
    "    def permute_add_featuremaps(self,feature_map):\n",
    "        '''\n",
    "        Perform the sum within output channels step.  (THIS NEEDS TO BE SPEED OPTIMIZED)\n",
    "        '''\n",
    "        x = feature_map\n",
    "        x = x[:, self.add_perm, :, :]\n",
    "        x = torch.split(x.unsqueeze(dim=1),self.in_channels,dim = 2)\n",
    "        x = torch.cat(x,dim = 1)\n",
    "        x = torch.sum(x,dim=2)\n",
    "        return x\n",
    "    \n",
    "    def gen_weight_ranks(self):\n",
    "        weight_ranks_flat = torch.abs(self.preadd_conv.weight).mean(dim=(2,3)).data.squeeze(1)\n",
    "        edge_weight_ranks = []\n",
    "        for o in self.add_indices:\n",
    "            in_chans = []\n",
    "            for i in self.add_indices[o]:\n",
    "                in_chans.append(weight_ranks_flat[i])\n",
    "            edge_weight_ranks.append(in_chans)\n",
    "        edge_weight_ranks = torch.tensor(edge_weight_ranks)\n",
    "        node_weight_ranks = edge_weight_ranks.mean(dim=1)\n",
    "        return weight_ranks_flat, node_weight_ranks\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, from_conv,store_activations=False, store_ranks = False, cuda=True):      # from conv is normal nn.Conv2d object to pull weights and bias from\n",
    "        super(dissected_Conv2d_edited, self).__init__()\n",
    "        self.from_conv = from_conv\n",
    "        self.in_channels = self.from_conv.weight.shape[1]\n",
    "        self.out_channels = self.from_conv.weight.shape[0]\n",
    "        self.cuda = cuda\n",
    "        self.store_activations = store_activations\n",
    "        self.store_ranks = store_ranks\n",
    "        self.postbias_ranks_prenorm = {'act':None,'grad':None,'actxgrad':None}\n",
    "        self.preadd_ranks_prenorm = {'act':None,'grad':None,'actxgrad':None}\n",
    "        self.images_seen = 0\n",
    "        self.weight_perm,self.add_perm,self.add_indices = self.gen_inout_permutation()\n",
    "        self.preadd_conv = self.make_preadd_conv()\n",
    "        self.bias = None\n",
    "        if self.from_conv.bias is not None:\n",
    "            self.bias = from_conv.bias.unsqueeze(1).unsqueeze(1)\n",
    "        #generate a dict that says which indices should be added together in for 'permute_add_featuremaps'\n",
    "\n",
    "\n",
    "        self.preadd_ranks_prenorm['weight'],self.postbias_ranks_prenorm['weight'] = self.gen_weight_ranks()\n",
    "        if self.store_ranks:\n",
    "            self.preadd_out_hook = None\n",
    "            self.postbias_out_hook = None\n",
    "\n",
    "    def compute_edge_rank(self,grad):\n",
    "        start = time.time()\n",
    "        activation = self.preadd_out\n",
    "        #activation_relu = F.relu(activation)\n",
    "        taylor = activation * grad \n",
    "        rank_key  = {'act':activation,'grad':grad,'actxgrad':taylor}\n",
    "        for key in rank_key:\n",
    "            if self.preadd_ranks_prenorm[key] is None: #initialize at 0\n",
    "                self.preadd_ranks_prenorm[key] = torch.FloatTensor(activation.size(1)).zero_()\n",
    "                if self.cuda:\n",
    "                    self.preadd_ranks_prenorm[key] = self.preadd_ranks_prenorm[key].cuda()\n",
    "            map_mean = rank_key[key].mean(dim=(2, 3)).data\n",
    "            mean_sum = map_mean.sum(dim=0).data      \n",
    "            self.postbias_ranks_prenorm[key] += mean_sum    # we sum up the mean activations over all images, after all batches\n",
    "            #have passed through we will average by the number of images seen with self.average_ranks\n",
    "        print('edge_rank time: %s'%str(time.time() - start))\n",
    "\n",
    "\n",
    "\n",
    "    def compute_node_rank(self,grad):\n",
    "        start = time.time()\n",
    "        activation = self.postbias_out\n",
    "        activation_relu = F.relu(activation)\n",
    "        taylor = activation * grad \n",
    "        rank_key  = {'act':activation_relu,'grad':grad,'actxgrad':taylor}\n",
    "        for key in rank_key:\n",
    "            if self.postbias_ranks_prenorm[key] is None: #initialize at 0\n",
    "                self.postbias_ranks_prenorm[key] = torch.FloatTensor(activation.size(1)).zero_()\n",
    "                if self.cuda:\n",
    "                    self.postbias_ranks_prenorm[key] = self.postbias_ranks_prenorm[key].cuda()\n",
    "            map_mean = rank_key[key].mean(dim=(2, 3)).data\n",
    "            mean_sum = map_mean.sum(dim=0).data      \n",
    "            self.postbias_ranks_prenorm[key] += mean_sum    # we sum up the mean activations over all images, after all batches\n",
    "            #have passed through we will average by the number of images seen with self.average_ranks\n",
    "        print('node_rank time: %s'%str(time.time() - start))\n",
    "\n",
    "\n",
    "\n",
    "    def format_edges(self, data= 'activations',prenorm=False):\n",
    "        #fetch preadd activations as [img,out_channel, in_channel,h,w]\n",
    "        #fetch preadd ranks as [out_chan,in_chan]\n",
    "\n",
    "        if not self.store_activations:\n",
    "            print('activations arent stored, use \"store_activations=True\" on model init. returning None')\n",
    "            return None\n",
    "        out_acts_list = []\n",
    "        if data == 'activations':\n",
    "            for out_chan in self.add_indices:\n",
    "                in_acts_list = []\n",
    "                for in_chan in self.add_indices[out_chan]:\n",
    "                    in_acts_list.append(self.preadd_out[:,in_chan,:,:].unsqueeze(dim=1).unsqueeze(dim=1))                    \n",
    "                out_acts_list.append(torch.cat(in_acts_list,dim=2))\n",
    "            return torch.cat(out_acts_list,dim=1).cpu().detach().numpy()\n",
    "\n",
    "        else:\n",
    "            output = {}\n",
    "            for rank_type in ['act','grad','actxgrad','weight']:\n",
    "                out_acts_list = []\n",
    "                for out_chan in self.add_indices:\n",
    "                    in_acts_list = []\n",
    "                    for in_chan in self.add_indices[out_chan]:\n",
    "                        if not prenorm:\n",
    "                            in_acts_list.append(self.preadd_ranks[rank_type][in_chan].unsqueeze(dim=0).unsqueeze(dim=0)) \n",
    "                        else:\n",
    "                            in_acts_list.append(self.preadd_ranks_prenorm[rank_type][in_chan].unsqueeze(dim=0).unsqueeze(dim=0))                 \n",
    "                    out_acts_list.append(torch.cat(in_acts_list,dim=1))\n",
    "                output[rank_type] = torch.cat(out_acts_list,dim=0).cpu().detach().numpy()\n",
    "            return output\n",
    "                        \n",
    "    def average_ranks(self):\n",
    "        for rank_type in ['act','grad','actxgrad']:\n",
    "            self.preadd_ranks_prenorm[rank_type] = self.preadd_ranks_prenorm[rank_type]/self.images_seen\n",
    "            self.postbias_ranks_prenorm[rank_type] = self.postbias_ranks_prenorm[rank_type]/self.images_seen\n",
    "\n",
    "\n",
    "    def normalize_ranks(self):\n",
    "        self.preadd_ranks = {}\n",
    "        self.postbias_ranks = {}\n",
    "        for rank_type in ['act','grad','actxgrad','weight']:\n",
    "            e = torch.abs(self.preadd_ranks_prenorm[rank_type])\n",
    "            n = torch.abs(self.postbias_ranks_prenorm[rank_type])\n",
    "\n",
    "            e = e.cpu()\n",
    "            e = e / np.sqrt(torch.sum(e * e))\n",
    "            n = n.cpu()\n",
    "            n = n / np.sqrt(torch.sum(n * n))\n",
    "\n",
    "            self.preadd_ranks[rank_type] = e\n",
    "            self.postbias_ranks[rank_type] = n\n",
    "\n",
    "        #self.preadd_ranks_prenorm['weight'] = self.preadd_ranks_prenorm['weight'].cpu()\n",
    "        #self.postbias_ranks_prenorm['weight'] = self.postbias_ranks_prenorm['weight'].cpu()\n",
    "        #self.preadd_ranks['weight'] = torch.abs(self.preadd_ranks_prenorm['weight'] )/np.sqrt(torch.sum(self.preadd_ranks_prenorm['weight'] *self.preadd_ranks_prenorm['weight'] ))\n",
    "        #self.postbias_ranks['weight'] = torch.abs(self.postbias_ranks_prenorm['weight'])/np.sqrt(torch.sum(self.postbias_ranks_prenorm['weight']*self.postbias_ranks_prenorm['weight']))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #pdb.set_trace()\n",
    "        self.images_seen += x.shape[0]    #keep track of how many images weve seen so we know what to divide by when we average ranks\n",
    "        if self.store_activations:\n",
    "            self.input = x\n",
    "\n",
    "        preadd_out = self.preadd_conv(x)  #get output of convolutions\n",
    "\n",
    "        #store values of intermediate outputs after convolution\n",
    "        if self.store_activations:\n",
    "            self.preadd_out = preadd_out\n",
    " \n",
    "        #Set hooks for calculating rank on backward pass\n",
    "        if self.store_ranks:\n",
    "            self.preadd_out = preadd_out\n",
    "            if self.preadd_out_hook is not None:\n",
    "                self.preadd_out_hook.remove()\n",
    "            self.preadd_out_hook = self.preadd_out.register_hook(self.compute_edge_rank)\n",
    "            #if self.preadd_ranks is not None:\n",
    "            #    print(self.preadd_ranks.shape)\n",
    "\n",
    "        added_out = self.permute_add_featuremaps(preadd_out)    #add convolution outputs by output channel\n",
    "        if self.bias is not None:  \n",
    "            postbias_out = added_out + self.bias\n",
    "        else:\n",
    "            postbias_out = added_out\n",
    "\n",
    "        #Store values of final module output\n",
    "        if self.store_activations:\n",
    "            self.postbias_out = postbias_out\n",
    " \n",
    "        #Set hooks for calculating rank on backward pass\n",
    "        if self.store_ranks:\n",
    "            self.postbias_out = postbias_out\n",
    "            if self.postbias_out_hook is not None:\n",
    "                self.postbias_out_hook.remove()\n",
    "            self.postbias_out_hook = self.postbias_out.register_hook(self.compute_node_rank)\n",
    "            #if self.postbias_ranks is not None:\n",
    "            #    print(self.postbias_ranks.shape)\n",
    "\n",
    "        return postbias_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "in_tensor = torch.rand([2,2,4,4])\n",
    "in_tensor = in_tensor.to('cuda')\n",
    "in_tensor = Variable(in_tensor)\n",
    "in_tensor.requires_grad_(True)\n",
    "\n",
    "\n",
    "normal_conv = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, padding=1,bias=False)\n",
    "normal_conv.to('cuda')\n",
    "\n",
    "dis_conv = dissected_Conv2d_edited(normal_conv,cuda=True,store_activations=True,store_ranks=False)\n",
    "dis_conv.to('cuda')\n",
    "\n",
    "flat_conv = nn.Conv2d(2,2*3,kernel_size = 3, \n",
    "                                 bias = False, padding=1,groups= 2)\n",
    "flat_conv.to('cuda')\n",
    "\n",
    "\n",
    "conv2 = nn.Conv2d(3,2,kernel_size = 3, \n",
    "                                 bias = False, padding=1,groups= 1)\n",
    "conv2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(in_tensor.grad)\n",
    "in_tensor.grad.zero_()\n",
    "in_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('flat')\n",
    "# start = time.time()\n",
    "# flat_out = flat_conv(in_tensor)\n",
    "# print(time.time()-start)\n",
    "\n",
    "# start = time.time()\n",
    "# torch.sum(flat_out).backward()\n",
    "# print(time.time()-start)\n",
    "\n",
    "\n",
    "print('normal')\n",
    "start = time.time()\n",
    "normal_out = normal_conv(in_tensor)\n",
    "#normal_hook = normal_out.register_hook(print_grad)\n",
    "print(time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "torch.sum(conv2(normal_out)).backward()\n",
    "print(time.time()-start)\n",
    "\n",
    "\n",
    "print(in_tensor.grad)\n",
    "#in_tensor.grad.zero_()\n",
    "\n",
    "\n",
    "print('dissected')\n",
    "start = time.time()\n",
    "dis_out = dis_conv(in_tensor)\n",
    "#dis_hook = dis_out.register_hook(print_grad)\n",
    "print(time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "torch.sum(conv2(dis_out)).backward()\n",
    "print(time.time()-start)\n",
    "\n",
    "print(in_tensor.grad)\n",
    "\n",
    "#normal_hook.remove()\n",
    "#dis_hook.remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dis_conv.preadd_out.shape)\n",
    "dis_conv.postbias_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_conv.zero_grad()\n",
    "dis_conv.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(normal_conv.weight.grad)\n",
    "dis_conv.preadd_conv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_conv.perm = torch.LongTensor([0,3,1,4,2,5])\n",
    "dis_conv.preadd_perm_out = dis_conv.preadd_out[:, dis_conv.perm, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dis_conv.preadd_out)\n",
    "\n",
    "print(dis_conv.preadd_perm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_conv.preadd_perm_split_out = torch.split(dis_conv.preadd_perm_out.unsqueeze(dim=1),2,dim = 2)\n",
    "dis_conv.preadd_perm_split_cat_out = torch.cat(dis_conv.preadd_perm_split_out,dim = 1)\n",
    "dis_conv.preadd_perm_split_cat_sum_out = torch.sum(dis_conv.preadd_perm_split_cat_out,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dis_conv.preadd_perm_split_out[0].shape)\n",
    "print(dis_conv.preadd_out.shape)\n",
    "print(dis_conv.preadd_perm_split_cat_out.shape)\n",
    "print(dis_conv.preadd_perm_split_cat_sum_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_out)\n",
    "print(dis_conv.preadd_perm_split_cat_sum_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4,2)\n",
    "print(a)\n",
    "torch.sum(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 5, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_model = model_classes.MNIST()\n",
    "mnist_model.load_state_dict(torch.load('../models/MNIST_statedict_0.993.pt'))\n",
    "\n",
    "mnist_model.conv1.weight.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning_viz",
   "language": "python",
   "name": "pruning_viz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
